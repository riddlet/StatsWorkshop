---
title: "Stats Workshop 2.0"
author:  Travis Riddle
Date: 1/XX/2015
output: 
  ioslides_presentation
---

## Strategy

Last time, we used Mark's data to illustrate loading data, some details about variable/object types, basic use of the `reshape2` package, and a few preliminary plots, made with ggplot.

Today, Maya has been nice enough to donate her data to the cause.  We're going to use her data to accomplish our objectives.  Specifically, we're going to

- Clean it up a bit
- Make some plots to discover some relationships
- think about ANOVA

## Material

Practically, we're going to rehash some of the stuff from last time.  As far as R is concerned, I don't think thre's anything in here that isn't stuff we didn't cover last time, but feel free to stop me with questions.

## Load in Data

In Maya's R script, she first loads in her data:
```{r}
srp <- read.csv('Data/SR.Past.Rel.12.10.14.csv')
```

After this, she goes on to calculate some index measures and clean the file up a bit.  I've omitted that code here.  However, I'll highlight one thing she did

```{r, echo=FALSE, message=FALSE}
#### EXCLUSIONS ####
#exclude 5 who copied the Michael text verbatim (exclcopied=1)
srp <- srp[ which(is.na(srp$exclcopied) == TRUE ), ]
#exclude 11 who used 'Michael' in their message (didn't follow directions) (exclmich=1)
srp <- srp[ which(is.na(srp$exclmich) == TRUE ), ]

####### COMPUTING VARIABLES ##########
attach(srp)
## rmq
srp$loc <- (rmq_1 + rmq_3 + rmq_4 + rmq_5 + rmq_8 + rmq_16 + rmq_21 + 
              rmq_25 + rmq_28 + rmq_29 + (7-rmq_13) + (7-rmq_24))/12
srp$ass <- ((7-rmq_2) + rmq_6 + rmq_7 + rmq_9 + (7-rmq_10) + rmq_11 + 
              rmq_15 + rmq_19 + rmq_20 + rmq_22 + (7-rmq_27) + rmq_30)/12
srp$locomassess = srp$loc-srp$ass

srp$prom = (6-rfq_1) + rfq_3 + rfq_7 + (6-rfq_9) + rfq_10 + (6-rfq_11)
srp$prev = (6-rfq_2) + (6-rfq_4) + rfq_5 + (6-rfq_6) + (6-rfq_8)
srp$prommprev = srp$prom-srp$prev

# closeness measure
srp$closeness <- (((8-close_1) + (8-close_2) + close_3 + close_4 + 
                     (8-close_5) + (8-close_6) + close_7 + close_8)/8)

# ntoBelong
srp$belong = ((6-NTB_1) + (6-NTB_3) + (6-NTB_7) + NTB_2 + NTB_4 + 
                NTB_5 + NTB_6 + NTB_8 + NTB_9 + NTB_10)/10

# Relational Trust
srp$reltrust = (reltrust_1 + reltrust_2 + reltrust_3 + reltrust_4)/4

# Epistemic Trust
srp$epistrust = (epistrust_1 + epistrust_2 + epistrust_3 + epistrust_4)/4

# message trust
srp$messtrust = (messtrust_1 + messtrust_2 + messtrust_3 + messtrust_4)/4

# ECR
srp$anx = (ECR.S_2 + ECR.S_4 + ECR.S_6 + (8-ECR.S_8) + ECR.S_10 + ECR.S_12)
srp$avoid = ((8-ECR.S_1) + ECR.S_3 + (8-ECR.S_5) + ECR.S_7 + ECR.S_9 + ECR.S_11)

#SCS
srp$independence = (((8-SCS_1) + (8-SCS_2) + (8-SCS_3) + (8-SCS_4) + 
                       (8-SCS_5) + (8-SCS_6) + (8-SCS_7) + (8-SCS_8) + 
                       (8-SCS_9) + (8-SCS_10) + (8-SCS_11) + (8-SCS_12) + 
                       SCS_13 + SCS_14 + SCS_15 + SCS_16 + SCS_17 + SCS_18 + 
                       SCS_19 + SCS_20 + SCS_21 + SCS_22 + SCS_23 + SCS_24)/24)

detach(srp)

#F-Scale
srp$authority = (((srp$F.Scale_1) + (srp$F.Scale_2) + (srp$F.Scale_3) + 
                    (srp$F.Scale_4) + (srp$F.Scale_5) + (srp$F.Scale_6) + 
                    (srp$F.Scale_7) + (srp$F.Scale_8) + (srp$F.Scale_9) + 
                    (srp$F.Scale_10) + (srp$F.Scale_11) + (srp$F.Scale_12) + 
                    srp$F.Scale_13 + srp$F.Scale_14 + srp$F.Scale_15 + 
                    srp$F.Scale_16 + srp$F.Scale_17 + srp$F.Scale_18 + 
                    srp$F.Scale_19 + srp$F.Scale_20 + srp$F.Scale_21 + 
                    srp$F.Scale_22)/22)


#### RECODING ####

####renaming manipulations 
#install.packages("reshape")
library(reshape)
library(reshape)
srp <- rename(srp, c(DO.BR.FL_19="relprime")) #dissolved=1, stable=2
srp <- rename(srp, c(DO.BR.FL_20="attitude")) #like=2, dislike=1
```


## Dummy coding manipulations

```{r}
srp$relprimedum[srp$relprime == "Stable Relationship Priming"] <- 0
srp$relprimedum[srp$relprime == "Dissolved Relationship Priming"] <- 1

srp$attitudedum[srp$attitude == "Michael-Dislike"] <- 0
srp$attitudedum[srp$attitude == "Michael-Like"] <- 1
```

Here, we're making a new variable which is a dummy code of another variable (i.e. code the two categories as 1 and zero).  Let's look a little more closely at what's going on here.

## Dummy coding manipulations

```{r, eval=FALSE}
srp$relprimedum[srp$relprime == "Stable Relationship Priming"] <- 0
srp$relprimedum[srp$relprime == "Dissolved Relationship Priming"] <- 1
```


> - We select the dataframe 

> - `srp` 

> - We indicate the column we want within that dataframe 

> - `srp$relprimedum` 

## Dummy coding manipulations

```{r, eval=FALSE}
srp$relprimedum[srp$relprime == "Stable Relationship Priming"] <- 0
srp$relprimedum[srp$relprime == "Dissolved Relationship Priming"] <- 1
```

`srp$relprimedum` is a column that didn't exist before, so when we do this, we are telling R to create a new variable with that name.


## Dummy coding manipulations

```{r, eval=FALSE}
srp$relprimedum[srp$relprime == "Stable Relationship Priming"] <- 0
srp$relprimedum[srp$relprime == "Dissolved Relationship Priming"] <- 1
```
So, new variable.  What now?

> - Fill it in with zeros! 

> - `srp$relprimedum ... <- 0`

## Dummy coding manipulations

```{r, eval=FALSE}
srp$relprimedum[srp$relprime == "Stable Relationship Priming"] <- 0
srp$relprimedum[srp$relprime == "Dissolved Relationship Priming"] <- 1
```
But there's other stuff present, right?  What's it doing?

`srp$relprimedum ... <- 0`

> - If we just ran the following, we would have a long vector of zeros

> - `srp$relprimedum <- 0`

## Dummy coding manipulations

```{r, eval=FALSE}
srp$relprimedum[srp$relprime == "Stable Relationship Priming"] <- 0
srp$relprimedum[srp$relprime == "Dissolved Relationship Priming"] <- 1
```
But zeros aren't that useful to us.  We want zeros and ones.  Specifically,  we want zeros for people who had stable relationship primes, and ones for those who had dissolved relationship priming.

This is what the bit in brackets is doing

> - `srp$relprime == "Stable Relationship Priming"`

> - This returns a bunch of `TRUE` and `FALSE` values.

> - When we plug these *boolean* values into the rows of the new variable that we're assigning values to (using bracket notation), it will only assign values to those that take a value of `TRUE`. 

## Subset & Plot

But is this process really necessary?  After this, Maya goes on to create subsets of her data so that she can plot them, like this:

```{r}
###### SUBSETTING ####
srlike <- subset(srp, attitudedum==1)
srdislike <- subset(srp, attitudedum==0)
srstable <- subset(srp, relprimedum==0)
srdissolve <- subset(srp, relprimedum==1)
```

## Subset & Plot

But is this process really necessary?  After this, Maya goes on to create subsets of her data so that she can plot them, like this:

```{r, echo=FALSE}
library(ggplot2)
theme_set(theme_bw(base_size = 14)) #for all graphs to have bw background & same font size
```

```{r, fig.align='center', fig.height=3.5}
ggplot(srlike, aes(x=vmessage)) + 
  geom_density()
```


## Subset & Plot

And then:

```{r, fig.align='center', fig.height=3.5}
ggplot(srdislike, aes(x=vmessage)) + 
  geom_density()
```

## Subset & Plot
Why do we plot?  To answer questions about the data..

> - Normally distributed? 
>     - Basically.
> - Difference between like & dislike? 
>     - Hard to tell...


## Subset & Plot
We can kill three birds with one stone here by:

1.  Just using the original variable (stored as a factor)

2.  Omitting the subsetting step

3.  Putting both conditions on the same plot

```{r}
str(srp$attitude)
```

R handles factors pretty intelligently.  We might have to fiddle with them on occasion, but in this case, it should work fine.

## Subset & Plot
```{r}
plot <- ggplot(srp, aes(x=vmessage, color=attitude))
plot + geom_density()
```

## More Plotting

What we're doing is examining the influence of two separate IV's on two DV's.  So, although we could make four plots like the one above (one for combination of IV & DV), we can simplify this even *more* by putting both IVs on the same plot.

## More Plotting
For the vRecall DV:
```{r, fig.height=4}
plot <- ggplot(srp, aes(x=vrecall, color=attitude))
plot + geom_density() + facet_wrap(~relprime)
```

## More Plotting
For the vMessage DV:
```{r, fig.height=4}
plot <- ggplot(srp, aes(x=vmessage, color=attitude))
plot + geom_density() + facet_wrap(~relprime)
```

## More Plotting
```{r, fig.height=3}
plot <- ggplot(srp, aes(x=vmessage, color=attitude))
plot + geom_density() + facet_wrap(~relprime)
```

The utitity of plotting shows itself!  That spike for dissolved priming certainly looks a little odd, don't you think?  Let's just make sure there's nothing weird about the data...

## Check the data {.smaller}
We can print out a nice table of the values:

```{r}
with(srp[srp$relprime == 'Dissolved Relationship Priming',], 
     table(vmessage, attitude))
```

## Check the data
So we saw that nine people selected the .5 option for that group.  This is certainly a lot, but I don't feel like it suggests that anything fishy is going on.

## Check the data
In Maya's original script, we next examined a correlation coefficient for the relationship between vrecall and vmessage

>  - Why?
>      - Presumably, because we're interested in the relationship


## Check the data {.smaller}

```{r}
cor.test(srstable$vmessage, srstable$vrecall) 
```

## Check the data {.smaller}

```{r}
cor.test(srdissolve$vmessage, srdissolve$vrecall) 
```

## Check the data

>  - What do we know?
>      - The data are significantly correlated.
>  - What do we not know?
>      - The nature of the correlation (i.e. what is the relationship?)

## Check the data {.smaller}

```{r fig.width=5, fig.height=4, warning=FALSE, fig.align='center', echo=FALSE}
library(png)
library(grid)
img <- readPNG('Images/Anscombe.png')
grid.raster(img)
```

>  - All four datasets have the same correlation.
>  - If we want to know about the relationship, we need to look at the data

## Check the data {.smaller}

Clear, strong, and positive relationship.  No funny business.
```{r, fig.height=3.5}
plot <- ggplot(srp, aes(x=vmessage, y=vrecall))
plot + geom_point(position=position_jitter())
```

Does this relationship differ by condition?

## Check the data {.smaller}

```{r, fig.height=3.5, fig.width=6}
plot <- ggplot(srp, aes(x=vmessage, y=vrecall))
plot + geom_point(position=position_jitter()) + facet_grid(attitude~relprime)
```

No difference in relationship, but there might be something else....

## Testing for inference with ANOVA {.smaller}

At this point, Maya computes a series of ANOVAs.  For example:
```{r}
## ONE-WAY ANOVAS: main effects
sr.att.mess <- aov(vmessage ~ attitudedum, data=srp)
summary(sr.att.mess) #main effect of attitude

sr.rel.mess <- aov(vmessage ~ relprimedum, data=srp)
summary(sr.rel.mess) #ns
```

## Testing for inference with ANOVA {.smaller}
```{r}
sr.rel.att.mess <- aov(vmessage ~ attitudedum * relprimedum, data=srp)
summary(sr.rel.att.mess) #main effect of attitude (interaction ns)
```

## Testing for inference with ANOVA

>  - This is one area where R simultaneously shines and is immensely frustrating to new users.
>  - Shines?
>      - Extremely flexible.
>      - You become intimately acquainted with precisely what you're doing with an ANOVA
>  - Frustrating
>      - You can't really 'plug and play'
>      - Easier to do something other than what you want (sometimes without realizing!)

## Testing for inference with ANOVA

Should the results of these two differ?

```{r}
m.1<-aov(vmessage ~ attitudedum * relprimedum, data=srp)
m.2<-aov(vmessage ~ relprimedum * attitudedum, data=srp)
```

## Testing for inference with ANOVA {.smaller}

Ah, but they do!
```{r}
summary(m.1)
summary(m.2)
```

##  ANOVA

Taking a step back, what are we doing with ANOVAs?

>  - **Anova** - **AN**alysis **O**f **VA**riance.
>      - Take some observed variability and analyze where it comes from.

>  - We have variability in `vmessage` - the valence of a message about *Micheal* participants sent to someone named *Sam*.
>  - Tory Higgins' "saying is believing" paradigm

##  ANOVA

>  - Where does the variability in `vmessage` come from?
>  - Partly from chance variation between people
>  - Partly because we've set up an experiment designed to induce variation.
>      - Some participants think that *Sam* likes *Michael*.  Others think (s)he doesn't.
>      - Some participants have been primed to think of a recently ended relationship.  Others were primed to think of an ongoing relationship.
>  - How much comes of the variability comes from chance, and how much from our manipulations?  And, between our manipulations, how much comes from manipulation of attitude vs. manipulation of relationship prime?

##  ANOVA

>  - This way of framing makes it seem like our manipulations are independent of each other.
>      - In other words, there's no relationship between our manipulation of attitude and our manipulation of the prime.
>          - if we could compute a correlation between our independent variables, it would be exactly zero!
>  - Surely this is what we meant to do.  But is it what we have *actually done*?
>  - Turns out, we can compute a correlation coefficient!

## ANOVA {.smaller}
1.  Assign numeric values to attitude and prime (for the sake of illustration) via `as.numeric()`
2.  Put these values into a new dataframe called `temp` and look at the first few rows with `head()`.
```{r}
temp <- data.frame(subj=srp$subj, attitude=as.numeric(srp$attitude), 
                   prime=as.numeric(srp$relprime))
head(temp)
```

>  - What's the correlation?
>  - ! 0
>  - that was a little programming joke....

##  ANOVA

```{r}
cor(as.numeric(srp$attitude), as.numeric(srp$relprime))
```

Close, but no cigar...

##  ANOVA & balance

This means we have what is commonly referred to as an 'unbalanced design'.

We have different numbers of observations in each of our 4 experimental cells.

Why would this matter?

##  This is the observed variability {.smaller}

```{r, echo=FALSE, fig.height=4, fig.width=5, fig.align='center'}
library(png)
library(grid)
img <- readPNG('Images/Var.png')
grid.raster(img)
```

>  - If our two variables are completely independent (i.e. uncorrelated) then they would each explain a unique portion of this observed variability

##  Our imaginary uncorrelated manipulations {.smaller}
```{r, echo=FALSE, fig.height=4, fig.width=5, fig.align='center'}
library(png)
library(grid)
img <- readPNG('Images/indep.png')
grid.raster(img)
```

>  - However, our two manipulations *aren't* inependent.  They're related to each other, so we get something different

##  Our actual, correlated manipulations {.smaller}
```{r, echo=FALSE, fig.height=4, fig.width=5, fig.align='center'}
library(png)
library(grid)
img <- readPNG('Images/Cor.png')
grid.raster(img)
```

>  - With regards to ANOVA, this means that we know that our experiment is responsible for all the variance taken up by the circles.
>  - The difficulty lies in the middle bit.  Which variable is it part of?

## Unbalanced Anovas

>  - We've inadvertently set up a situation in which our manipulations are fighting to explain the same piece of the variance lasagna.
>  - The question of what to do with this shared piece is answered by how we choose to allocate our Sums of squares.
>  - 3 types
>      - Type I SS ('sequential')
>      - Type II SS ('heirarchical')
>      - Type III SS ('marginal')

## Type I SS

>  - Breaks up the estimation sequentially
>      1. SS(Prime)
>      2. SS(Attitude | Prime)
>      3. SS(Prime * Attitude | Prime, Attitude)

>  - Let's visualize this

## Step One: get the variance associated with Prime

```{r, echo=FALSE, fig.align='center', fig.height=4, fig.width=5} 
source('..//colorfulVennPlot/R/plotVenn3d.R')  #load in my modified version
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c('#144256', rep('White', 2), rep('#144256', 2),
                             'White', '#144256'), 
                  printvals=F)
```

## Step Two: Get the remaining bit of Attitude

```{r, echo=FALSE, fig.align='center', fig.height=4, fig.width=5}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c('#144256', rep('#88301B', 2), rep('#144256', 2),
                             'White', '#144256'), printvals=F)
```

## Step Three: Take the remaining variance left in the interaction term

```{r, echo=FALSE, fig.align='center', fig.height=4, fig.width=5}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c('#144256', rep('#88301B', 2), rep('#144256', 2), 
                             '#88691B', '#144256'), printvals=F)
```
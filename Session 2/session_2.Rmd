---
title: "Session 2"
output: html_document
---

Last time, we spent some effort looking at Mark's data.  This time, Maya has been kind enought to donate some of her data for the greater good.  She's already sent me her csv as well as the script she's used for the work.  I'm going to zip through some of her pre-processing first.  I'll stop to highlight a couple of things along the way, though.  We're doing all this with the objective of describing how to plot to find relationships (both how to do it and what to look for) as well as how to run a basic ANOVA and what doing such a thing means.  But first, a word about outliers.

### Outliers
There seems to be a fair amount of ambiguity around how an analyst ought to deal with outliers.  In particular, there are a few options which are often suggested:

1.  Exclude them.
2.  Ignore that they exist.
3.  Run analyses with and without and pray that it doesn't make any difference.

None of these are wrong, *per se*, but I think applying any kind of blanket rule to the question of what to do about outliers is, in general, not a wise choice.  If you're confronted with a situation in which you have a data point or two that are much different than the rest of your data, the best thing you can do is to think carefully about *why* they are different.  The answer might be in your data, it might be in notes to your experiment, or it might require some detective work of another sort altogether.  Your answer to the question of why a particular observation is an outlier should also lead you toward what to do about it.

For instance, if a data point is different because it has been mis-entered (e.g. a research assistant typed 300 rather than 30), you may then decide to fix the problem in one of two ways - either fix it or remove it.  Fixing it will only be possible if you are *sure* of what the real value should be.

On the other hand, you might have an observation which is different just because the person who contributed that datapoint behaved *much* differently than anyone else.  In this scenario, the question you might ask yourself is, 'did this person experience the same experiment that everyone else did?'  If the answer to that is yes, then it shouldn't matter if they behaved much differently than everyone else - their experience is just as meaningful to your research as is someone who scored much closer to the center of the distribution.  If, on the other hand, they experienced a different experiment - perhaps they didn't follow instructions, or the experimenter made a mistake - then in this case, you should almost certainly exclude this data.

Essentially, you never want to throw away good information.  If your outlier is good information, then you should keep it.  You might consider some type of transformation of the data such that your models are better behaved (e.g. take the natural log).

##Maya's data

```{r, warning=FALSE, message=FALSE}
srp <- read.csv('../Data/SR.Past.Rel.12.10.14.csv')
#### EXCLUSIONS ####
#exclude 5 who copied the Michael text verbatim (exclcopied=1)
srp <- srp[ which(is.na(srp$exclcopied) == TRUE ), ]
#exclude 11 who used 'Michael' in their message (didn't follow directions) (exclmich=1)
srp <- srp[ which(is.na(srp$exclmich) == TRUE ), ]

####### COMPUTING VARIABLES ##########
attach(srp)
## rmq
srp$loc <- (rmq_1 + rmq_3 + rmq_4 + rmq_5 + rmq_8 + rmq_16 + rmq_21 + 
              rmq_25 + rmq_28 + rmq_29 + (7-rmq_13) + (7-rmq_24))/12
srp$ass <- ((7-rmq_2) + rmq_6 + rmq_7 + rmq_9 + (7-rmq_10) + rmq_11 + 
              rmq_15 + rmq_19 + rmq_20 + rmq_22 + (7-rmq_27) + rmq_30)/12
srp$locomassess = srp$loc-srp$ass

srp$prom = (6-rfq_1) + rfq_3 + rfq_7 + (6-rfq_9) + rfq_10 + (6-rfq_11)
srp$prev = (6-rfq_2) + (6-rfq_4) + rfq_5 + (6-rfq_6) + (6-rfq_8)
srp$prommprev = srp$prom-srp$prev

# closeness measure
srp$closeness <- (((8-close_1) + (8-close_2) + close_3 + close_4 + 
                     (8-close_5) + (8-close_6) + close_7 + close_8)/8)

# ntoBelong
srp$belong = ((6-NTB_1) + (6-NTB_3) + (6-NTB_7) + NTB_2 + NTB_4 + 
                NTB_5 + NTB_6 + NTB_8 + NTB_9 + NTB_10)/10

# Relational Trust
srp$reltrust = (reltrust_1 + reltrust_2 + reltrust_3 + reltrust_4)/4

# Epistemic Trust
srp$epistrust = (epistrust_1 + epistrust_2 + epistrust_3 + epistrust_4)/4

# message trust
srp$messtrust = (messtrust_1 + messtrust_2 + messtrust_3 + messtrust_4)/4

# ECR
srp$anx = (ECR.S_2 + ECR.S_4 + ECR.S_6 + (8-ECR.S_8) + ECR.S_10 + ECR.S_12)
srp$avoid = ((8-ECR.S_1) + ECR.S_3 + (8-ECR.S_5) + ECR.S_7 + ECR.S_9 + ECR.S_11)

#SCS
srp$independence = (((8-SCS_1) + (8-SCS_2) + (8-SCS_3) + (8-SCS_4) + 
                       (8-SCS_5) + (8-SCS_6) + (8-SCS_7) + (8-SCS_8) + 
                       (8-SCS_9) + (8-SCS_10) + (8-SCS_11) + (8-SCS_12) + 
                       SCS_13 + SCS_14 + SCS_15 + SCS_16 + SCS_17 + SCS_18 + 
                       SCS_19 + SCS_20 + SCS_21 + SCS_22 + SCS_23 + SCS_24)/24)

detach(srp)

#F-Scale
srp$authority = (((srp$F.Scale_1) + (srp$F.Scale_2) + (srp$F.Scale_3) + 
                    (srp$F.Scale_4) + (srp$F.Scale_5) + (srp$F.Scale_6) + 
                    (srp$F.Scale_7) + (srp$F.Scale_8) + (srp$F.Scale_9) + 
                    (srp$F.Scale_10) + (srp$F.Scale_11) + (srp$F.Scale_12) + 
                    srp$F.Scale_13 + srp$F.Scale_14 + srp$F.Scale_15 + 
                    srp$F.Scale_16 + srp$F.Scale_17 + srp$F.Scale_18 + 
                    srp$F.Scale_19 + srp$F.Scale_20 + srp$F.Scale_21 + 
                    srp$F.Scale_22)/22)


#### RECODING ####

####renaming manipulations 
#install.packages("reshape")
library(reshape)
library(reshape)
srp <- rename(srp, c(DO.BR.FL_19="relprime")) #dissolved=1, stable=2
srp <- rename(srp, c(DO.BR.FL_20="attitude")) #like=2, dislike=1
```

The first place I'll stop is here.  Maya is creating a couple of new variables, both coded as zero and one.

```{r, eval=FALSE}

# dummy coding manipulations
srp$relprimedum[srp$relprime == "Stable Relationship Priming"] <- 0
srp$relprimedum[srp$relprime == "Dissolved Relationship Priming"] <- 1

srp$attitudedum[srp$attitude == "Michael-Dislike"] <- 0
srp$attitudedum[srp$attitude == "Michael-Like"] <- 1
```

However, this might not be necessary.  I haven't looked ahead in her code, but R is pretty good at dealing with factor variables.  It usually knows what to do with them and one usually need not explicitly code them as zero and one.  After doing this, she goes on to create several new subsets of data, as defined by these factor variables, so that she can make some histograms of vmessage and vrecall.

```{r, eval=FALSE}
###### SUBSETTING ####
srlike <- subset(srp, attitudedum==1)
srdislike <- subset(srp, attitudedum==0)
srstable <- subset(srp, relprimedum==0)
srdissolve <- subset(srp, relprimedum==1)

library(ggplot2)
theme_set(theme_bw(base_size = 14)) #for all graphs to have bw background & same font size

ggplot(srlike, aes(x=vmessage)) + 
  geom_density()
ggplot(srdislike, aes(x=vmessage)) + 
  geom_density()
ggplot(srstable, aes(x=vmessage)) + 
  geom_density()
ggplot(srdissolve, aes(x=vmessage)) + 
  geom_density()

ggplot(srlike, aes(x=vrecall)) + 
  geom_density()
ggplot(srdislike, aes(x=vrecall)) + 
  geom_density()
ggplot(srstable, aes(x=vrecall)) + 
  geom_density()
ggplot(srdissolve, aes(x=vrecall)) + 
  geom_density()
```

As far as letting us look at our variables to see if they're normally distributed within these subgroups, this approach works okay.  However, the whole thing could be dramatically simplified, and also give us the added bonus of allowing us to see how these distributions change by group.  This is a similar approach to what we were doing with Mark's data.

```{r}
library(ggplot2)
plot <- ggplot(srp, aes(x=vmessage, color=relprime))
plot + geom_density()
```

I've used the original relprime variable to split the distribution into two density curves.  No need to factor, subset, and create separate plots.  We can do the same for vrecall, and we can also repeat the whole thing for attitude:

```{r}
plot <- ggplot(srp, aes(x=vrecall, color=relprime))
plot + geom_density()
```

```{r}
plot <- ggplot(srp, aes(x=vmessage, color=attitude))
plot + geom_density()
```

```{r}
plot <- ggplot(srp, aes(x=vrecall, color=attitude))
plot + geom_density()
```

Now, since we seem to be splitting the data according to the attitude and relprime variable, I'm guessing these are manipulations or covariates that we're interested in.  Let's simplify this plotting even further by faceting the plot.

```{r}
plot <- ggplot(srp, aes(x=vrecall, color=attitude))
plot + geom_density() + facet_wrap(~relprime)
```

```{r}
plot <- ggplot(srp, aes(x=vmessage, color=attitude))
plot + geom_density() + facet_wrap(~relprime)
```

Oookay, that last one is a bit weird.  Seems like nearly 1/2 of that subset of the sample must have given the same rating?

```{r}
table(srp$vmessage, srp$attitude, srp$relprime)
```

Yeah, this lists 9 people who used .5 in that offending group.  Although unusual, it doesn't seem like anything fishy is going on here.

Next, Maya computes some correlation coefficients.  I think the idea here is right, and you'll want to know these coefficients for writing up the results, but what we're seeking here is *understanding*, right?  I've always felt better about looking at data, rather than figuring out what a one-number summary means.  Time to bust out our good friend, the scatter plot.  First, the overall pattern.  I've added a slight jitter to the points so that we can see if any are plotted right on top of each other (a common problem when dealing with scales which have only a few response options)

```{r}
plot <- ggplot(srp, aes(x=vmessage, y=vrecall))
plot + geom_point(position=position_jitter())
```

A pretty clear, strong, positive relationship.  Now let's look at the same pattern by subsets:

```{r}
plot <- ggplot(srp, aes(x=vmessage, y=vrecall))
plot + geom_point(position=position_jitter()) + facet_grid(attitude~relprime)
```

All of these seem to be similar to the overall pattern.  Maybe Dissolved/Dislike is a slightly weaker relationship than the others?  This also shows that the Like attitude seems to have boosted both variables, don't you think?  See how the points are clustered closer to the upper right for the bottom two plots?  Again, this might not pan out in the proper analyses, but if something is going to show up, that's probably going to be it.

Maya next computes a series of ANOVAs.  

```{r, eval=FALSE}
## ONE-WAY ANOVAS: main effects
sr.att.mess <- aov(vmessage ~ attitudedum, data=srp)
summary(sr.att.mess) #main effect of attitude

sr.att.rec <- aov(vrecall ~ attitudedum, data=srp)
summary(sr.att.rec) #main effect of attitude

sr.rel.mess <- aov(vmessage ~ relprimedum, data=srp)
summary(sr.rel.mess) #ns

sr.rel.rec <- aov(vrecall ~ relprimedum, data=srp)
summary(sr.rel.rec) #ns

## TWO-WAY ANOVAS: interactions
sr.rel.att.mess <- aov(vmessage ~ attitudedum * relprimedum, data=srp)
summary(sr.rel.att.mess) #main effect of attitude (interaction ns)

sr.rel.att.rec <- aov(vrecall ~ attitudedum * relprimedum, data=srp)
summary(sr.rel.att.rec) #main effect of attitude (interaction ns)
```

This is gonna get ugly.  It's time to consider what we're doing here, and how to specify it.  So, what is an ANOVA?

###ANOVA
It stands for Analysis Of VAriance.  That is, we're taking some observed variability, and figuring out where it comes from.  For simplicity, let's focus on `vmessage`.  I believe this variable reflects the valence of the message that participants produced about someone named *Michael*.  They then sent this message to someone named *Sam*.  For you social cognitive researchers in the room, this is a standard *saying-is-believing* paradigm a-la Tory Higgins.  Regardless, it should be obvious that participants will generate messages which are not exactly equally valenced.  Indeed, we've already seen this in some of the plots we've made.  What we'd like to know is where this variability comes from.  

We've set up an experiment in which we think that two manipulations will induce some variance.  Specifically, we've made it such that some participants think that Sam likes Michael, and others think that Sam dislikes Michael.  We've also primed some people to think of either a recently ended relationship or a relationship that is ongoing.  How much of the variability in `vmessage` is due to the contribution of these two manipulations?

We're thinking of these two variables as completely independent from each other.  That is, if we could compute a correlation coefficient between them, it would be exactly zero.  It turns out, we *can* compute a correlation coefficient!  Look at the data:

```{r}
temp <- data.frame(subj=srp$subj, attitude=as.numeric(srp$attitude), 
                   prime=as.numeric(srp$relprime))
head(temp)
```


We've assigned numeric values to attitude and prime (for the sake of illustration) via `as.numeric()`.  We then put them in a new dataframe called `temp` and looked at the first few rows with `head()`.  What's the correlation between them?  Well, it isn't zero.  

```{r}
cor(as.numeric(srp$attitude), as.numeric(srp$relprime))
```


It's `r round(cor(as.numeric(srp$attitude), as.numeric(srp$relprime)), 5)`.  I know, I know.  *Almost*, right?  Well, almost isn't good enough.  We need zero on the dot.

Why does this matter?  Remember that we're trying to explain the variability in `vmessage`.  We'd like to attribute some of that variability to attitude and some to the prime.  Visually, the variability is usually depicted as a geometric box.

![Variance][img1]

If our two variabiles were totally independent, then we could imagine that they each explain some unique portion of that variance.

![Independent][img2]

However, we know that our two variables are *not* independent.  They are related to each other.  So our situation looks a little more like this:

![Correlated][img3]

What this means for our ANOVA is that we need to figure out how to explain all the variance in our box which is taken up by circles.  In particular, what are we to do with the middle bit?  Is it part of Prime?  Part of attitude?  Part of both?  Neither?  We've inadvertently set up a situation in which our two explanatory variables will be fighting to explain the same piece of variance.

The answer to the question of what to do about the overlapping bit is determined by which Sum of Squares (SS) we select, of which there are three basic types

#### Type I SS
Type I SS breaks up the estimates sequentially:

1.  SS(Prime)
2.  SS(Attitude | Prime)
3.  SS(Prime*Attitude | Prime, Attitude)

In other words, we get all the variance explained by Prime in step 1, then the variance associated with Attitude after we've taken out the .00071 chunk in step 2, and finally, the variance associated with the interaction, after removing Prime and Attitude in step 3.  Below, I've included the interaction term.  I left it out above for pedagogical reasons, but it works in the same way.

To visualize, in step one, we get all the variance associated with Prime:

```{r, message=FALSE}
# Note I'm using a modified version of the colorfulVennPlot package.
# The modified package can be found on my github page.
source('../..//colorfulVennPlot/R/plotVenn3d.R')
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c('#144256', rep('White', 2), rep('#144256', 2),
                             'White', '#144256'), 
                  printvals=F)
```


Then, step two, we take the remaining bit of Attitude:

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c('#144256', rep('#88301B', 2), rep('#144256', 2),
                             'White', '#144256'), printvals=F)
```

In step three, we just take the remaining variance left in the interaction term:

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c('#144256', rep('#88301B', 2), rep('#144256', 2), 
                             '#88691B', '#144256'), printvals=F)
```

When Maya used `aov()`, she was using type I SS.  We can see this by examining what happens if we switch the order in which we enter the terms:

```{r}
sr.rel.att.mess1 <- aov(vmessage ~ attitude * relprime, data=srp)
sr.rel.att.mess2 <- aov(vmessage ~ relprime * attitude, data=srp)
summary(sr.rel.att.mess1) #main effect of attitude (interaction ns)
summary(sr.rel.att.mess2) #main effect of attitude (interaction ns)
```

The differences are not huge, but they are present.  Note, for example, the change in the F-value for our two main effects.  As you can guess, this is not typically what we want when examining an experiment like the one Maya ran.  We usually think of main effects as 'the influence of $IV_1$, holding $IV_2$ constant (sometimes said, 'Influence of $IV_1$, accounting for $IV_2$).

#### Type II SS
Type II resolves this issue, in a fashion.  It gives us the estimates for each main effect, while controlling for the other main effect(s).  That is:  

 - SS(Prime | Attitude)  
 - SS(Attitude | Prime)  
 - SS(Prime * Attitude | Prime, Attitude)  

Let's revisit the those plots of variance accounted for:

```{r, message=FALSE}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c(rep('White', 7)), printvals=F)
```

Now we have a visual representation of our full model.  Now, let's look at what portion of the variance is explained by each of our terms.  First, *SS(Prime | Attitude)*

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c('#144256', rep('White', 2), '#144256', rep('White', 3) ), printvals=F)
```

In plain terms, you can say that we've estimated the amount of total variance accounted for in Prime, removing the variance associated with attitude.  Next *SS(Attitude | Prime)*

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c('#144256', rep('#88301B', 2), '#144256', rep('White', 3) ), printvals=F)
```

Again, in plain terms, we've estimated the amount of total variance accounted for in Attitude, removing the variance associated with Prime.  Finally, *SS(Prime * Attitude | Prime, Attitude)*

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c('#144256', rep('#88301B', 2), '#144256', 'White', 
                             '#88691B', 'White'), printvals=F)
```

Here, we've estimated the amount of variance accounted for by the Prime*Attitude interaction, removing the variance associated with the main effects of Prime and Attitude.  

Why would we want to do this?  Well, as you can see, it gives a lot of power to detect any main effects that might be present, as a large portion of the main effect circles are colored.  However, some of that area is eating into the interaction.  That is, you're saying that you'd rather spend that area detecting a main effect than detecting an interaction.  If this is true, then why are you bothering modeling an interaction in the first place?  If you use type II SS and get a significant interaction term, then that should lead you to question what that means for your main effects, because it isn't clear what to do with the bits that overlap with the interaction term  (should they be part of the interaction?  The main effect?  neither?).

In practice, type II SS are almost never used because of this issue with what's going on in the interaction term.  Some people advocate using this when there's no interaction present, because it gives you better power to detect main effects, but (I think that) this is a relatively small group.

To do a type II in R, we should use the `Anova()` function from the `car` package.  There ~~might be~~ is a way to do it without, but I don't know it, it's probably a bit more complicated:

```{r}
library(car)
sr.rel.att.mess <- aov(vmessage ~ attitude * relprime, data=srp)
a.sr<-Anova(sr.rel.att.mess, type=2)
a.sr
```

It's interesting to note here that the outcome for the two main effects are the same as if we had taken the estimates in step 2 from each of our type I anovas.  One can think about that visually too.  This makes sense, because step 2 in type I anova is the effect of a particular IV, controlling/accounting for the variability in the other one.

#### Type III SS
Type III SS is, historically, the most commonly used in psychology (e.g. the default in SPSS, SAS, & STATA).  This is because we're most typically interested in the effect of every term in our model, controlling for the others.  In our current example:

 - SS(Prime | Attitude, Prime * Attitude)  
 - SS(Attitude | Prime, Prime * Attitude)  
 - SS(Prime * Attitude | Prime, Attitude) 
 
This differs from type II SS because we're now only interested in the bit of variance explained that is *unique* to each particular term.  Visually:

*SS(Prime | Attitude, Prime * Attitude)*

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c(rep('White', 3), '#144256', rep('White', 3) ), 
                  printvals=F)
```

You can see that we're only interested in those pieces of the variance that we are *sure* belong to Prime.  Same for *SS(Prime | Attitude, Prime * Attitude)*:

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c(rep('White', 2), '#88301B', '#144256',
                             rep('White', 3)), printvals=F)
```

Finally, the same holds true for the interaction term, *SS(Prime * Attitude | Attitude, Prime)*:

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c(rep('White', 2), '#88301B', '#144256',
                             'White', '#88691B', 'White'), printvals=F)
```

What's the advantage of this approach?  

Well, for one thing, it is relatively conservative.  That is, since we're only looking at the variance that is explained by our model in an unambiguous way, there will never be any confusion over whether some main effect that we're seeing is real, or is actually part of an interaction term (like could potentially happen in type II).  In other words, with type II, we are biased toward detecting main effects, at the expense of detecting any interaction.  This can be especially problematic when the interaction is exactly what is causing the main effect!  A set of results like this, for instance:

```{r}
y<-c(2, 2, 3, 8) #fake means
x<-c('A', 'B', 'A', 'B') # IV 1
z<-c('C', 'C', 'D', 'D') # IV 2
df.plot<-data.frame(x, y, z) # put them in a data frame
plot <- ggplot(df.plot, aes(x=x, y=y, group=z, color=z))
  plot + geom_point(size=7)+
  geom_line()
```

If our data looked like this, and we used a type II sums of squares, we would be more likely to interpret what is clearly an interaction (i.e. x influences Y, but only if you're in category D) as a main effect (i.e. Y is higher for category B than category A).

Note, by the way, that many of these problems just don't exist if we **LOOK AT OUR DATA**, and use hypothesis testing as a way of confirming what we already suspect is happening.  At any rate, how do we go about conducting a type III test in R?  There's one additional step in comparison to the type II.

You see, R is pretty good about automatically handling factors.  This means that we don't typically have to take the extra step of making contrasts (i.e. dummy coded) versions of our variables, as R does it for us.  In order to do this, R relies on a set of internal functions.  As you may or may not remember from some other statistics class, there are any number of ways to create contrasts.  R has some default way of doing it that depends on what kind of factor you're feeding it:

```{r}
getOption('contrasts')
```

R has two types of factors - *unordered*, such as the one's we're dealing with here, and *ordered*, like you might see when administering High, Medium, and Low dosages of a drug.  The above tells us that unordered factors are contrasted using the contr.treatment function, and ordered are contrasted using the contr.poly function.

Let's see how our two variables are coded with the current settings:

```{r}
with(srp, contrasts(attitude))
with(srp, contrasts(relprime))
```

We get a pretty straightforward dummy variable coding.  This works for regression, but when running an ANOVA, we should use effects coding (e.g. making sure the values sum to zero).  The reasoning behind this should probably be saved for a different workshop, but we can easily set it so that this is what we get in R:

```{r}
options(contrasts=c('contr.sum', 'contr.poly'))
with(srp, contrasts(relprime))
```

Note that we're feeding the contrast options two different values - one for contr.sum, and one for contr.poly.  These two values correspond to the two functions used for unordered and ordered factor variable types, respectively.

Once we've done this, we can proceed as we did for a type II, except this time we specify that we are interested in the type III SS:

```{r}
sr.rel.att.mess <- aov(vmessage ~ attitude * relprime, data=srp)
a.sr<-Anova(sr.rel.att.mess, type=3)
a.sr
```

Note that all of this stuff about different sums of squares is really only relevant for unbalanced designs (i.e. differenct observations in different cells).  If you've got a balanced design, then all three methods will give you the same answer, because then your IVs are not correlated, and there's no issue with determinging what variance each variable is explaining.

###Recap

##### When you have an outlier, think about why it is what it is.

Don't throw away good information.

##### Look at your data!

Really can't emphasize this enough.

##### The 3 flavors of sums of squares

*Type I* is sequential.  The estimates you get will depend on the order in which you neter the terms.

*Type II* is biased towards detecting main effects at the expense of interactions, but has more power to do so

*Type III* does not have the bias present in type II, but has the disadvantage of having the least power of any of them (i.e. is the most conservative).  You will almost always be expected to run a type III.

[img1]: ../Images/Var.png
[img2]: ../Images/indep.png
[img3]: ../Images/cor.png


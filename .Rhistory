xmlName(wiki.root[[1]][[1]][[2]])
xmlName(wiki.root[[1]][[1]][[3]])
xmlAttrs(wiki.root[[1]][[1]])
xmlAttrs(wiki.root[[1]][[1]][[1]])
xmlAttrs(wiki.root[[1]][[1]][[1]])
xmlAttrs(wiki.root[[1]][[1]][[2]])
xmlAttrs(wiki.root[[1]][[1]][[3]])
Users
wiki.doc <- xmlTreeParse('/Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml') #read in the xml document
library(XML)
wiki.doc <- xmlTreeParse('/Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml') #read in the xml document
wiki.root <- xmlRoot(wiki.doc) #get the root node
xmlApply(wiki.root[[1]][[1]], xmlAttrs)
names(wiki.root)
xmlName(wiki.root)
names(wiki.root[[1]])
names(wiki.root[[1]][[1]])
names(wiki.root[[1]][[1]][[1]])
names(wiki.root[[1]][[1]][[2]])
names(wiki.root[[1]][[1]][[3]])
names(wiki.root[[1]][[2]][[3]])
names(wiki.root[[1]][[3]][[3]])
xmlSApply(wiki.root[[1]][[1]], xmlValue)
xmlSApply(wiki.root[[1]], xmlValue)
a<-xmlSApply(wiki.root[[1]], xmlValue)
a[1]
xmlApply(wiki.root[[1]], xmlValue)
b<-xmlApply(wiki.root[[1]], xmlValue)
?xmlSApply
a
a<-xmlSApply(wiki.root[[1]][[1]], xmlValue)
a
Now, what I really want is a dataframe in which each row is a sentence, and there are a number of variables associated with each sentence, including certainty, and which article it came from.
getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
a<-getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
names(a)
a[1]
a[[1]]
a[[1]][[1]]
a[[1]][[2]]
a[[1]]
a[[8925]]
a[[8924]]
a[[8923]]
a[[8922]]
a[[8921]]
a[[8920]]
a[[8919]]
a[[8918]]
a[[8917]]
a[[8916]]
a[[8915]]
a[[8914]]
a[[8913]]
a[[8912]]
a[[8911]]
a[[8910]]
a[[8909]]
a[[8908]]
a[[8907]]
a[[8906]]
a[[8905]]
a[[8904]]
a[[8903]]
a[[8902]]
a[[8901]]
a[[8900]]
a[[8899]]
a[[8898]]
a[[8897]]
a[[8896]]
a[[8895]]
a[[8894]]
a[[8893]]
a[[8892]]
a[[8891]]
a[[8890]]
a[[8889]]
a[[8888]]
a[[8887]]
a[[8886]]
a[[8885]]
a[[8884]]
a[[8883]]
a[[8882]]
a[[8881]]
a[[8880]]
a[[8879]]
a[[8878]]
a[[8877]]
a[[8876]]
a[[8875]]
a[[8874]]
a[[8873]]
a[[8872]]
a[[8871]]
a[[8870]]
?gsub
gsub('<ccue>', '', wiki.doc)
gsub(c('<ccue>', '</ccue>'), '', wiki.doc)
wiki.doc.cuestripped<-gsub('</?ccue>', '', wiki.doc)
head(a)
c<-gsub('</?ccue>', '', a)
head(c)
c<-lapply(a, fun=function(x) gsub('</?ccue>', '', x))
c<-lapply(a, FUN=function(x) gsub('</?ccue>', '', x))
head(c)
text<-readLines('Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', encoding='UTF-8')
text<-gsub('</?ccue>', '', text)
text <- readLines('Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', encoding='UTF-8')
text<-gsub('</?ccue>', '', text)
wiki.doc <- xmlTreeParse(text)
library(XML)
wiki.doc <- xmlTreeParse(text)
wiki.root <- xmlRoot(wiki.doc)
sentencenodes <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
sentencenodes[1]
sentencenodes[1][1]
sentences <- lapply(sentencenodes, function(x) xmlSApply(x, xmlValue))
sentences[1]
sentences[2]
sentences[3]
sentences[8925]
certainty.values <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence/@certainty')
certainty.values <- xpathApply(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence/@certainty', xmlAttrs)
certainty.values <- xpathApply(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence', xmlAttrs)
certainty.values <- xpathApply(wiki.root, '//Document/DocumentPart[@type="Text"]', xmlAttrs)
certainty.values <- xpathApply(wiki.root, '/Document', xmlAttrs)
certainty.values <- xpathApply(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence[@certainty]', xmlGetAttr, 'certainty')
certainty.values <- xpathApply(wiki.root, "//Document/DocumentPart[@type='Text']/sentence[@certainty]", xmlGetAttr, 'certainty')
wiki.root <- xmlRoot(wiki.doc, useInternalNodes=T) #get the root node
certainty.values <- xpathApply(wiki.root, "//Document/DocumentPart[@type='Text']/sentence[@certainty]", xmlGetAttr, 'certainty')
certainty.values <- xpathApply(wiki.root, "//Document/DocumentPart[@type='Text']/sentence", xmlGetAttr, 'certainty')
wiki.doc <- xmlParse('/Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml') #read in the xml document
wiki.root <- xmlRoot(wiki.doc, useInternalNodes=T) #get the root node
type(wiki.root)
str(wiki.root)
xmlName(wiki.root)
xmlSize(wiki.root)
xmlName(wiki.root[[1]])
xmlSize(wiki.root[[1]])
xmlName(wiki.root[[1]][[1]])
xmlSize(wiki.root[[1]][[1]])
xmlAttrs(wiki.root[[1]][[1]])
xmlApply(wiki.root[[1]][[1]], xmlName)
xmlApply(wiki.root[[1]][[1]], xmlAttrs)
text <- readLines('/Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', encoding='UTF-8')
text<-gsub('</?ccue>', '', text)
wiki.doc <- xmlParse(text)
wiki.root <- xmlRoot(wiki.doc)
sentencenodes <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
sentences <- lapply(sentencenodes, function(x) xmlSApply(x, xmlValue))
certainty.values <- xpathApply(wiki.root, "//Document/DocumentPart[@type='Text']/sentence", xmlGetAttr, 'certainty')
head(certainty.values)
library(tm)
intall.packages('tm')
install.packages('tm')
library(tm)
inspect(sentences)
?inspect
Corpus(sentences)
?VectorSource
doc.vec<-VectorSource(sentences)
doc.vec[1]
doc.vec[2]
doc.vec[3]
doc.vec[4]
doc.vec[5]
inspect(doc.vec)
doc.corpus <- Corpus(doc.vec)
summary(doc.coprus)
summary(doc.corpus)
doc.vec<-VectorSource(unlist(sentences))
doc.corpus <- Corpus(doc.vec)
summary(doc.corpus)
str(wiki.doc)
getSources()
readXML(spec = list('node' '//Document/DocumentPart[@type="Text"]/sentence',))
readXML(spec = list('node' '//Document/DocumentPart[@type="Text"]/sentence',
readXML(spec = list(sentence = list('node', '//Document/DocumentPart[@type="Text"]/sentence')
certainty = list('attribute', '//Document/DocumentPart[@type='Text']/sentence')))
readXML(spec = list(sentence = list('node', '//Document/DocumentPart[@type="Text"]/sentence'),
certainty = list('attribute', '//Document/DocumentPart[@type='Text']/sentence')))
readXML(spec = list(sentence = list('node', '//Document/DocumentPart[@type="Text"]/sentence'),
certainty = list('attribute', '//Document/DocumentPart[@type="Text"]/sentence')))
readXML(spec = list(sentence = list('node', '//Document/DocumentPart[@type="Text"]/sentence'),
certainty = list('attribute', '//Document/DocumentPart[@type="Text"]/sentence')), doc=wiki.root)
readXML(spec = list(sentence = list('node', '//Document/DocumentPart[@type="Text"]/sentence'),
certainty = list('attribute', '//Document/DocumentPart[@type="Text"]/sentence')), doc=text)
xml.file <- system.file('//Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml')
readXML(spec = list(sentence = list('node', '//Document/DocumentPart[@type="Text"]/sentence'),
certainty = list('attribute', '//Document/DocumentPart[@type="Text"]/sentence')), doc=xml.file)
xml.file <- system.file('//Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', package = 'tm')
readXML(spec = list(sentence = list('node', '//Document/DocumentPart[@type="Text"]/sentence'),
certainty = list('attribute', '//Document/DocumentPart[@type="Text"]/sentence')), doc=xml.file)
print(readLines(xml.file), quote = F)
data.frame(unlist(sentences), unlist(certainty.values))
wiki.df<-data.frame(unlist(sentences), unlist(certainty.values))
head(wiki.df)
tail(wiki.df)
names(wiki.df)
names(wiki.df) <- c('Sentence', 'Certainty')
names(wiki.df)
doc.vec <- VectorSource(wiki.df$Sentence)
doc.corpus <- Corpus(doc.vec)
summary(doc.corpus)
inspect(doc.corpus)
inspect(doc.corpus[1])
TDM <- termDocumentMatrix(doc.corpus)
TDM <- TermDocumentMatrix(doc.corpus)
TDM
findAssocs(TDM, "love", .8)
findAssocs(TDM, "love", .5)
findAssocs(TDM, "thought", .5)
findAssocs(TDM, "thought", .25)
?tm_map
library(SnowballC)
install.packages('SnowballC')
wiki.df <- data.frame(sentences = unlist(sentences), certainty = unlist(certainty.values))
doc.vec <- VectorSource(wiki.df$sentences)
doc.corpus <- Corpus(doc.vec)
doc.corpus <- tm_map(doc.corpus, content_transformer(tolower))
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
doc.corpus <- tm_map(doc.corpus, stemDocument)
inspect(doc.corpus[1])
inspect(doc.corpus[2])
head(wiki.df)
inspect(doc.corpus[1])
TDM <- TermDocumentMatrix(doc.corpus)
TDM
summary(TDM)
summarize(TDM)
names(TDM)
TDM$i
?TDM
?TermDocumentMatrix
inspect(TDM)
inspect(TDM[200:205, 1:5])
inspect(TDM[1:10, 1:5])
inspect(TDM[1:15, 1:5])
inspect(TDM[1:25, 1:5])
inspect(TDM[1:25, 1:10])
inspect(TDM[1:50, 1:15])
inspect(TDM[1:75, 1:20])
inspect(TDM[1:75, 1:25])
inspect(TDM[10:100, 1:25])
text <- readLines('/Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', encoding='UTF-8')
text<-gsub('</?ccue>', '', text)
wiki.doc <- xmlParse(text)
library(XML)
library(tm)
library(SnowballC)
wiki.doc <- xmlParse(text)
wiki.root <- xmlRoot(wiki.doc)
sentencenodes <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
sentences <- lapply(sentencenodes, function(x) xmlSApply(x, xmlValue))
certainty.values <- xpathApply(wiki.root, "//Document/DocumentPart[@type='Text']/sentence", xmlGetAttr, 'certainty')
wiki.df <- data.frame(sentences = unlist(sentences), certainty = unlist(certainty.values))
doc.vec <- VectorSource(wiki.df$sentences)
doc.corpus <- Corpus(doc.vec)
doc.corpus <- tm_map(doc.corpus, content_transformer(tolower))
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
doc.corpus <- tm_map(doc.corpus, stemDocument)
TDM <- TermDocumentMatrix(doc.corpus)
inspect(TDM[100:125, 1:20]) #edit to find a nice segment to display.
inspect(TDM[125:150, 1:20]) #edit to find a nice segment to display.
inspect(TDM[150:125, 1:20]) #edit to find a nice segment to display.
inspect(TDM[150:175, 1:20]) #edit to find a nice segment to display.
inspect(TDM[160:185, 1:20]) #edit to find a nice segment to display.
inspect(TDM[169:194, 1:20]) #edit to find a nice segment to display.
inspect(TDM)
summary(TDM)
names(TDM)
TDM[4]
TDM$nrow
table(certainty.values)
head(wiki.df)
table(wiki.df$certainty)
DTM <- DocumentTermMatrix(doc.corpus)
inspect(DTM[169:194, 1:20])
inspect(DTM[1:20, 169:194])
inspect(DTM[1:25, 169:189])
inspect(DTM[5:30, 169:184])
inspect(DTM[5:30, 169:180])
DTM$dim
DTM$nrow
DTM$ncol
table(DTM > 1)
names(DTM)
head(DTM$i)
head(DTM$j)
length(DTM$i)
length(DTM$j)
length(DTM$v)
meta(DTM)
findFreqTerms(DTM, 5)
findFreqTerms(DTM, 10)
findFreqTerms(DTM, 15)
findFreqTerms(DTM, 20)
findFreqTerms(DTM, 25)
findFreqTerms(DTM, 30)
findFreqTerms(DTM, 50)
findFreqTerms(DTM, 100)
findAssoc(DTM, 'power', .5)
findAssocs(DTM, 'power', .5)
findAssocs(DTM, 'power', .25)
findAssocs(DTM, 'power', .1)
findAssocs(DTM, 'caus', .15)
inspect(removeSparseTerms(DTM, .5))
inspect(removeSparseTerms(DTM, .9))
inspect(removeSparseTerms(DTM, .5)[1:25, 1:25])
inspect(removeSparseTerms(DTM, .5)[1:10, 1:10])
?create_matrix
install.packages('RTextTools')
library(RTextTools)
?create_matrix
a<-create_matrix(wiki.df$sentences, language='english', ngramLength=c(1, 2, 3), removeNumbers = T, removePunctuation = T, removeSparseTerms = T, removeStopwords = T, stemWords=T, toLower=T)
?RTextTools
?textcnt
a<-create_matrix(wiki.df$sentences, language='english', ngramLength=1, removeNumbers = T, removePunctuation = T, removeSparseTerms = T, removeStopwords = T, stemWords=T, toLower=T)
a<-create_matrix(wiki.df$sentences, language='english', ngramLength=1, removeNumbers = T, removePunctuation = T, removeStopwords = T, stemWords=T, toLower=T)
b<-create_matrix(wiki.df$sentences, language='english', ngramLength=2, removeNumbers = T, removePunctuation = T, removeStopwords = T, stemWords=T, toLower=T)
b<-create_matrix(wiki.df$sentences, language='english', ngramLength=2, removeNumbers = T, removePunctuation = T, removeStopwords = T, stemWords=T, toLower=T)
b<-create_matrix(wiki.df$sentences, language='english', ngramLength=3, removeNumbers = T, removePunctuation = T, removeStopwords = T, stemWords=T, toLower=T)
bigramtokenizer <- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))
DTM.bigram <- DocumentTermMatrix(doc.corpus, control = list(tokenize = bigramtokenizer))
DTM.bigram <- TermDocumentMatrix(doc.corpus, control = list(tokenize = bigramtokenizer))
b<-create_matrix(wiki.df$sentences, language='english', ngramLength=3)
detach("package:SnowballC", unload=TRUE)
library("SnowballC", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
b<-create_matrix(wiki.df$sentences, language='english', ngramLength=3)
install.packages('SnowballC')
install.packages("SnowballC")
library(SnowballC)
b<-create_matrix(wiki.df$sentences, language='english', ngramLength=3)
b<-create_matrix(doc.corpus, language='english', ngramLength=3)
text <- readLines('/Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', encoding='UTF-8')
text<-gsub('</?ccue>', '', text)
sentencenodes <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
library(XML)
sentencenodes <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
sentences <- lapply(sentencenodes, function(x) xmlSApply(x, xmlValue))
wiki.doc <- xmlParse(text)
wiki.root <- xmlRoot(wiki.doc)
sentencenodes <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
sentences <- lapply(sentencenodes, function(x) xmlSApply(x, xmlValue))
certainty.values <- xpathApply(wiki.root, "//Document/DocumentPart[@type='Text']/sentence", xmlGetAttr, 'certainty')
wiki.df <- data.frame(sentences = unlist(sentences), certainty = unlist(certainty.values))
doc.vec <- VectorSource(wiki.df$sentences)
library(tm)
doc.vec <- VectorSource(wiki.df$sentences)
doc.corpus <- Corpus(doc.vec)
doc.corpus <- tm_map(doc.corpus, content_transformer(tolower))
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
doc.corpus <- tm_map(doc.corpus, stemDocument)
doc.corpus <- tm_map(doc.corpus, str_replace_all,"[^[:alnum:]]", "")
bigramtokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min=2, max=2))}
tdm <- TermDocumentMatrix(doc.corpus, control = list(tokenize=bigramtokenizer))
tdm <- TermDocumentMatrix(wiki.df$sentences, control = list(tokenize=bigramtokenizer))
data(crude)
test <- TermDocumentMatrix(crude, control = list(tokenize = bigramtokenizer))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
library("RWeka")
library("tm")
data("crude")
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
library("RWeka")
library("tm")
data("crude")
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
options(mc.cores=1)
library("RWeka")
library("tm")
data("crude")
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
tdm
inspect(tdm)
library(tm)
BigramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2))}
options(mc.cores=1)
data(crude)
txtTdmBi <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
inspect(txtTdmBi[1:20, 1:20])
library(XML)
text <- readLines('/Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', encoding='UTF-8')
text <- iconv(text, to='utf-8')
text <- readLines('/Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', encoding='UTF-8')
text<-gsub('</?ccue>', '', text)
wiki.doc <- xmlParse(text)
wiki.root <- xmlRoot(wiki.doc)
sentencenodes <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
sentences <- lapply(sentencenodes, function(x) xmlSApply(x, xmlValue))
certainty.values <- xpathApply(wiki.root, "//Document/DocumentPart[@type='Text']/sentence", xmlGetAttr, 'certainty')
wiki.df <- data.frame(sentences = unlist(sentences), certainty = unlist(certainty.values))
doc.vec <- VectorSource(wiki.df$sentences)
doc.corpus <- Corpus(doc.vec)
doc.corpus <- tm_map(doc.corpus, content_transformer(tolower))
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
doc.corpus <- tm_map(doc.corpus, stemDocument)
txtTdmBi <- TermDocumentMatrix(doc.corpus, control = list(tokenize = BigramTokenizer))
inspect(txtTdmBi[1:20, 1:20])
inspect(txtTdmBi[1:200, 1:200])
uniTDM <- TermDocumentMatrix(doc.corpus)
TrigramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3))}
biTDM <- TermDocumentMatrix(doc.corpus, control = list(tokenize = BigramTokenizer))
triTDM <- TermDocumentMatrix(doc.corpus, control = list(tokenize = TrigramTokenizer))
inspect(TDM[1:30, 1:30])
TDM <- TermDocumentMatrix(doc.corpus)
inspect(TDM[1:30, 1:30])
inspect(TDM[30:60, 30:60])
inspect(TDM[60:90, 60:90])
inspect(TDM[90:120, 90:120])
inspect(TDM[1:30, 90:120])
inspect(TDM[1:30, 30:60])
inspect(TDM[1:30, 60:90])
inspect(TDM[30:60, 60:90])
inspect(TDM[30:60, 30:60])
inspect(TDM[30:60, 1:30])
inspect(TDM[60:90, 1:30])
inspect(TDM[60:90, 30:60])
inspect(TDM[60:90, 60:90])
TDM[1, 1]
TDM
max(TDM)
TDM>8
hist(TDM)
hist(unlist(TDM))
biTDM[1:30, 1:30]
biTDM[1:30, 30:60]
biTDM
biTDM[1:30, 60:90]
biTDM[30:60, 60:90]
biTDM[30:60, 30:60]
biTDM[30:60, 1:30]
inspect(biTDM[30:60, 1:30])
sentences[18]
triDTM[1:30, 1:30]
triTDM[1:30, 1:30]
triTDM[1:60, 1:30]
triTDM[30:60, 1:30]
triTDM[30:90, 1:30]
triTDM[30:90, 1:60]
triTDM[30:90, 1:90]
triTDM[30:1200, 1:90]
triTDM[30:120, 1:90]
triTDM[30:150, 1:90]
triTDM[30:180, 1:90]
triTDM[30:210, 1:90]
triTDM[30:240, 1:90]
triTDM[30:270, 1:90]
triTDM[30:270,]
TDM
110921/158129329
biTDM
109623/882787077
triTDM
101248/888668102
biTDM[60:90, 60:90]
biTDM[60:120, 60:90]
biTDM[30:120, 60:90]
biTDM[30:120, 1:30]
biTDM[30:90, 1:30]
biTDM[30:60, 1:30]
biTDM[40:60, 1:30]
biTDM[30:50, 1:30]
inspect(biTDM[30:50, 1:30])
inspect(biTDM[30:50, 10:30])
triTDM[1:100, 1:100]
triTDM[1:50, 1:100]
triTDM[1:25, 1:100]
triTDM[30:50, 1:100]
triTDM[30:50, 1:50]
triTDM[30:50, 1:20]
inspect(triTDM[30:50, 1:20])
test<-c(TDM, biTDM, triTDM)
test
tdm[1:10, 1:100]
TDM[1:10, 1:100]
inspect(TDM[1:10, 1:100])
inspect(TDM[1:10, 8900:9000])
inspect(TDM[1:10, 8900])
inspect(TDM[1:10, 8920:8930])
inspect(TDM[1:10, 8925])
inspect(TDM[1:10, 8926])
inspect(test[1:10, 8926])
inspect(test[1:10, 8925])
1/60
breach <- read.csv('Data/breachmeans.csv')
setwd("/Volumes/triddle/Teaching/Grad stats sem")
breach <- read.csv('Data/breachmeans.csv')
breach <- read.csv('/Data/breachmeans.csv')
ls
ls()
getwd()
breach <- read.csv('Data/breachmeans.csv')
ls
getwd()
setwd("/Volumes/triddle/Teaching/Grad stats sem/StatsWorkshop")
ls
breach <- read.csv('Data/breachmeans.csv')

a[[8888]]
a[[8887]]
a[[8886]]
a[[8885]]
a[[8884]]
a[[8883]]
a[[8882]]
a[[8881]]
a[[8880]]
a[[8879]]
a[[8878]]
a[[8877]]
a[[8876]]
a[[8875]]
a[[8874]]
a[[8873]]
a[[8872]]
a[[8871]]
a[[8870]]
?gsub
gsub('<ccue>', '', wiki.doc)
gsub(c('<ccue>', '</ccue>'), '', wiki.doc)
wiki.doc.cuestripped<-gsub('</?ccue>', '', wiki.doc)
head(a)
c<-gsub('</?ccue>', '', a)
head(c)
c<-lapply(a, fun=function(x) gsub('</?ccue>', '', x))
c<-lapply(a, FUN=function(x) gsub('</?ccue>', '', x))
head(c)
text<-readLines('Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', encoding='UTF-8')
text<-gsub('</?ccue>', '', text)
text <- readLines('Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', encoding='UTF-8')
text<-gsub('</?ccue>', '', text)
wiki.doc <- xmlTreeParse(text)
library(XML)
wiki.doc <- xmlTreeParse(text)
wiki.root <- xmlRoot(wiki.doc)
sentencenodes <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
sentencenodes[1]
sentencenodes[1][1]
sentences <- lapply(sentencenodes, function(x) xmlSApply(x, xmlValue))
sentences[1]
sentences[2]
sentences[3]
sentences[8925]
certainty.values <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence/@certainty')
certainty.values <- xpathApply(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence/@certainty', xmlAttrs)
certainty.values <- xpathApply(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence', xmlAttrs)
certainty.values <- xpathApply(wiki.root, '//Document/DocumentPart[@type="Text"]', xmlAttrs)
certainty.values <- xpathApply(wiki.root, '/Document', xmlAttrs)
certainty.values <- xpathApply(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence[@certainty]', xmlGetAttr, 'certainty')
certainty.values <- xpathApply(wiki.root, "//Document/DocumentPart[@type='Text']/sentence[@certainty]", xmlGetAttr, 'certainty')
wiki.root <- xmlRoot(wiki.doc, useInternalNodes=T) #get the root node
certainty.values <- xpathApply(wiki.root, "//Document/DocumentPart[@type='Text']/sentence[@certainty]", xmlGetAttr, 'certainty')
certainty.values <- xpathApply(wiki.root, "//Document/DocumentPart[@type='Text']/sentence", xmlGetAttr, 'certainty')
wiki.doc <- xmlParse('/Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml') #read in the xml document
wiki.root <- xmlRoot(wiki.doc, useInternalNodes=T) #get the root node
type(wiki.root)
str(wiki.root)
xmlName(wiki.root)
xmlSize(wiki.root)
xmlName(wiki.root[[1]])
xmlSize(wiki.root[[1]])
xmlName(wiki.root[[1]][[1]])
xmlSize(wiki.root[[1]][[1]])
xmlAttrs(wiki.root[[1]][[1]])
xmlApply(wiki.root[[1]][[1]], xmlName)
xmlApply(wiki.root[[1]][[1]], xmlAttrs)
text <- readLines('/Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', encoding='UTF-8')
text<-gsub('</?ccue>', '', text)
wiki.doc <- xmlParse(text)
wiki.root <- xmlRoot(wiki.doc)
sentencenodes <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
sentences <- lapply(sentencenodes, function(x) xmlSApply(x, xmlValue))
certainty.values <- xpathApply(wiki.root, "//Document/DocumentPart[@type='Text']/sentence", xmlGetAttr, 'certainty')
head(certainty.values)
library(tm)
intall.packages('tm')
install.packages('tm')
library(tm)
inspect(sentences)
?inspect
Corpus(sentences)
?VectorSource
doc.vec<-VectorSource(sentences)
doc.vec[1]
doc.vec[2]
doc.vec[3]
doc.vec[4]
doc.vec[5]
inspect(doc.vec)
doc.corpus <- Corpus(doc.vec)
summary(doc.coprus)
summary(doc.corpus)
doc.vec<-VectorSource(unlist(sentences))
doc.corpus <- Corpus(doc.vec)
summary(doc.corpus)
str(wiki.doc)
getSources()
readXML(spec = list('node' '//Document/DocumentPart[@type="Text"]/sentence',))
readXML(spec = list('node' '//Document/DocumentPart[@type="Text"]/sentence',
readXML(spec = list(sentence = list('node', '//Document/DocumentPart[@type="Text"]/sentence')
certainty = list('attribute', '//Document/DocumentPart[@type='Text']/sentence')))
readXML(spec = list(sentence = list('node', '//Document/DocumentPart[@type="Text"]/sentence'),
certainty = list('attribute', '//Document/DocumentPart[@type='Text']/sentence')))
readXML(spec = list(sentence = list('node', '//Document/DocumentPart[@type="Text"]/sentence'),
certainty = list('attribute', '//Document/DocumentPart[@type="Text"]/sentence')))
readXML(spec = list(sentence = list('node', '//Document/DocumentPart[@type="Text"]/sentence'),
certainty = list('attribute', '//Document/DocumentPart[@type="Text"]/sentence')), doc=wiki.root)
readXML(spec = list(sentence = list('node', '//Document/DocumentPart[@type="Text"]/sentence'),
certainty = list('attribute', '//Document/DocumentPart[@type="Text"]/sentence')), doc=text)
xml.file <- system.file('//Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml')
readXML(spec = list(sentence = list('node', '//Document/DocumentPart[@type="Text"]/sentence'),
certainty = list('attribute', '//Document/DocumentPart[@type="Text"]/sentence')), doc=xml.file)
xml.file <- system.file('//Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', package = 'tm')
readXML(spec = list(sentence = list('node', '//Document/DocumentPart[@type="Text"]/sentence'),
certainty = list('attribute', '//Document/DocumentPart[@type="Text"]/sentence')), doc=xml.file)
print(readLines(xml.file), quote = F)
data.frame(unlist(sentences), unlist(certainty.values))
wiki.df<-data.frame(unlist(sentences), unlist(certainty.values))
head(wiki.df)
tail(wiki.df)
names(wiki.df)
names(wiki.df) <- c('Sentence', 'Certainty')
names(wiki.df)
doc.vec <- VectorSource(wiki.df$Sentence)
doc.corpus <- Corpus(doc.vec)
summary(doc.corpus)
inspect(doc.corpus)
inspect(doc.corpus[1])
TDM <- termDocumentMatrix(doc.corpus)
TDM <- TermDocumentMatrix(doc.corpus)
TDM
findAssocs(TDM, "love", .8)
findAssocs(TDM, "love", .5)
findAssocs(TDM, "thought", .5)
findAssocs(TDM, "thought", .25)
?tm_map
library(SnowballC)
install.packages('SnowballC')
wiki.df <- data.frame(sentences = unlist(sentences), certainty = unlist(certainty.values))
doc.vec <- VectorSource(wiki.df$sentences)
doc.corpus <- Corpus(doc.vec)
doc.corpus <- tm_map(doc.corpus, content_transformer(tolower))
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
doc.corpus <- tm_map(doc.corpus, stemDocument)
inspect(doc.corpus[1])
inspect(doc.corpus[2])
head(wiki.df)
inspect(doc.corpus[1])
TDM <- TermDocumentMatrix(doc.corpus)
TDM
summary(TDM)
summarize(TDM)
names(TDM)
TDM$i
?TDM
?TermDocumentMatrix
inspect(TDM)
inspect(TDM[200:205, 1:5])
inspect(TDM[1:10, 1:5])
inspect(TDM[1:15, 1:5])
inspect(TDM[1:25, 1:5])
inspect(TDM[1:25, 1:10])
inspect(TDM[1:50, 1:15])
inspect(TDM[1:75, 1:20])
inspect(TDM[1:75, 1:25])
inspect(TDM[10:100, 1:25])
text <- readLines('/Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', encoding='UTF-8')
text<-gsub('</?ccue>', '', text)
wiki.doc <- xmlParse(text)
library(XML)
library(tm)
library(SnowballC)
wiki.doc <- xmlParse(text)
wiki.root <- xmlRoot(wiki.doc)
sentencenodes <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
sentences <- lapply(sentencenodes, function(x) xmlSApply(x, xmlValue))
certainty.values <- xpathApply(wiki.root, "//Document/DocumentPart[@type='Text']/sentence", xmlGetAttr, 'certainty')
wiki.df <- data.frame(sentences = unlist(sentences), certainty = unlist(certainty.values))
doc.vec <- VectorSource(wiki.df$sentences)
doc.corpus <- Corpus(doc.vec)
doc.corpus <- tm_map(doc.corpus, content_transformer(tolower))
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
doc.corpus <- tm_map(doc.corpus, stemDocument)
TDM <- TermDocumentMatrix(doc.corpus)
inspect(TDM[100:125, 1:20]) #edit to find a nice segment to display.
inspect(TDM[125:150, 1:20]) #edit to find a nice segment to display.
inspect(TDM[150:125, 1:20]) #edit to find a nice segment to display.
inspect(TDM[150:175, 1:20]) #edit to find a nice segment to display.
inspect(TDM[160:185, 1:20]) #edit to find a nice segment to display.
inspect(TDM[169:194, 1:20]) #edit to find a nice segment to display.
inspect(TDM)
summary(TDM)
names(TDM)
TDM[4]
TDM$nrow
table(certainty.values)
head(wiki.df)
table(wiki.df$certainty)
DTM <- DocumentTermMatrix(doc.corpus)
inspect(DTM[169:194, 1:20])
inspect(DTM[1:20, 169:194])
inspect(DTM[1:25, 169:189])
inspect(DTM[5:30, 169:184])
inspect(DTM[5:30, 169:180])
DTM$dim
DTM$nrow
DTM$ncol
table(DTM > 1)
names(DTM)
head(DTM$i)
head(DTM$j)
length(DTM$i)
length(DTM$j)
length(DTM$v)
meta(DTM)
findFreqTerms(DTM, 5)
findFreqTerms(DTM, 10)
findFreqTerms(DTM, 15)
findFreqTerms(DTM, 20)
findFreqTerms(DTM, 25)
findFreqTerms(DTM, 30)
findFreqTerms(DTM, 50)
findFreqTerms(DTM, 100)
findAssoc(DTM, 'power', .5)
findAssocs(DTM, 'power', .5)
findAssocs(DTM, 'power', .25)
findAssocs(DTM, 'power', .1)
findAssocs(DTM, 'caus', .15)
inspect(removeSparseTerms(DTM, .5))
inspect(removeSparseTerms(DTM, .9))
inspect(removeSparseTerms(DTM, .5)[1:25, 1:25])
inspect(removeSparseTerms(DTM, .5)[1:10, 1:10])
?create_matrix
install.packages('RTextTools')
library(RTextTools)
?create_matrix
a<-create_matrix(wiki.df$sentences, language='english', ngramLength=c(1, 2, 3), removeNumbers = T, removePunctuation = T, removeSparseTerms = T, removeStopwords = T, stemWords=T, toLower=T)
?RTextTools
?textcnt
a<-create_matrix(wiki.df$sentences, language='english', ngramLength=1, removeNumbers = T, removePunctuation = T, removeSparseTerms = T, removeStopwords = T, stemWords=T, toLower=T)
a<-create_matrix(wiki.df$sentences, language='english', ngramLength=1, removeNumbers = T, removePunctuation = T, removeStopwords = T, stemWords=T, toLower=T)
b<-create_matrix(wiki.df$sentences, language='english', ngramLength=2, removeNumbers = T, removePunctuation = T, removeStopwords = T, stemWords=T, toLower=T)
b<-create_matrix(wiki.df$sentences, language='english', ngramLength=2, removeNumbers = T, removePunctuation = T, removeStopwords = T, stemWords=T, toLower=T)
b<-create_matrix(wiki.df$sentences, language='english', ngramLength=3, removeNumbers = T, removePunctuation = T, removeStopwords = T, stemWords=T, toLower=T)
bigramtokenizer <- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))
DTM.bigram <- DocumentTermMatrix(doc.corpus, control = list(tokenize = bigramtokenizer))
DTM.bigram <- TermDocumentMatrix(doc.corpus, control = list(tokenize = bigramtokenizer))
b<-create_matrix(wiki.df$sentences, language='english', ngramLength=3)
detach("package:SnowballC", unload=TRUE)
library("SnowballC", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
b<-create_matrix(wiki.df$sentences, language='english', ngramLength=3)
install.packages('SnowballC')
install.packages("SnowballC")
library(SnowballC)
b<-create_matrix(wiki.df$sentences, language='english', ngramLength=3)
b<-create_matrix(doc.corpus, language='english', ngramLength=3)
text <- readLines('/Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', encoding='UTF-8')
text<-gsub('</?ccue>', '', text)
sentencenodes <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
library(XML)
sentencenodes <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
sentences <- lapply(sentencenodes, function(x) xmlSApply(x, xmlValue))
wiki.doc <- xmlParse(text)
wiki.root <- xmlRoot(wiki.doc)
sentencenodes <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
sentences <- lapply(sentencenodes, function(x) xmlSApply(x, xmlValue))
certainty.values <- xpathApply(wiki.root, "//Document/DocumentPart[@type='Text']/sentence", xmlGetAttr, 'certainty')
wiki.df <- data.frame(sentences = unlist(sentences), certainty = unlist(certainty.values))
doc.vec <- VectorSource(wiki.df$sentences)
library(tm)
doc.vec <- VectorSource(wiki.df$sentences)
doc.corpus <- Corpus(doc.vec)
doc.corpus <- tm_map(doc.corpus, content_transformer(tolower))
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
doc.corpus <- tm_map(doc.corpus, stemDocument)
doc.corpus <- tm_map(doc.corpus, str_replace_all,"[^[:alnum:]]", "")
bigramtokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min=2, max=2))}
tdm <- TermDocumentMatrix(doc.corpus, control = list(tokenize=bigramtokenizer))
tdm <- TermDocumentMatrix(wiki.df$sentences, control = list(tokenize=bigramtokenizer))
data(crude)
test <- TermDocumentMatrix(crude, control = list(tokenize = bigramtokenizer))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
txtTdmBi <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
library("RWeka")
library("tm")
data("crude")
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
library("RWeka")
library("tm")
data("crude")
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
options(mc.cores=1)
library("RWeka")
library("tm")
data("crude")
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
tdm
inspect(tdm)
library(tm)
BigramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2))}
options(mc.cores=1)
data(crude)
txtTdmBi <- TermDocumentMatrix(crude, control = list(tokenize = BigramTokenizer))
inspect(txtTdmBi[1:20, 1:20])
library(XML)
text <- readLines('/Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', encoding='UTF-8')
text <- iconv(text, to='utf-8')
text <- readLines('/Users/triddle/Desktop/hedge detection/AffirmationHedging/task1_train_wikipedia_rev2.xml', encoding='UTF-8')
text<-gsub('</?ccue>', '', text)
wiki.doc <- xmlParse(text)
wiki.root <- xmlRoot(wiki.doc)
sentencenodes <- getNodeSet(wiki.root, '//Document/DocumentPart[@type="Text"]/sentence')
sentences <- lapply(sentencenodes, function(x) xmlSApply(x, xmlValue))
certainty.values <- xpathApply(wiki.root, "//Document/DocumentPart[@type='Text']/sentence", xmlGetAttr, 'certainty')
wiki.df <- data.frame(sentences = unlist(sentences), certainty = unlist(certainty.values))
doc.vec <- VectorSource(wiki.df$sentences)
doc.corpus <- Corpus(doc.vec)
doc.corpus <- tm_map(doc.corpus, content_transformer(tolower))
doc.corpus <- tm_map(doc.corpus, removePunctuation)
doc.corpus <- tm_map(doc.corpus, removeNumbers)
doc.corpus <- tm_map(doc.corpus, removeWords, stopwords("english"))
doc.corpus <- tm_map(doc.corpus, stemDocument)
txtTdmBi <- TermDocumentMatrix(doc.corpus, control = list(tokenize = BigramTokenizer))
inspect(txtTdmBi[1:20, 1:20])
inspect(txtTdmBi[1:200, 1:200])
uniTDM <- TermDocumentMatrix(doc.corpus)
TrigramTokenizer <- function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3))}
biTDM <- TermDocumentMatrix(doc.corpus, control = list(tokenize = BigramTokenizer))
triTDM <- TermDocumentMatrix(doc.corpus, control = list(tokenize = TrigramTokenizer))
inspect(TDM[1:30, 1:30])
TDM <- TermDocumentMatrix(doc.corpus)
inspect(TDM[1:30, 1:30])
inspect(TDM[30:60, 30:60])
inspect(TDM[60:90, 60:90])
inspect(TDM[90:120, 90:120])
inspect(TDM[1:30, 90:120])
inspect(TDM[1:30, 30:60])
inspect(TDM[1:30, 60:90])
inspect(TDM[30:60, 60:90])
inspect(TDM[30:60, 30:60])
inspect(TDM[30:60, 1:30])
inspect(TDM[60:90, 1:30])
inspect(TDM[60:90, 30:60])
inspect(TDM[60:90, 60:90])
TDM[1, 1]
TDM
max(TDM)
TDM>8
hist(TDM)
hist(unlist(TDM))
biTDM[1:30, 1:30]
biTDM[1:30, 30:60]
biTDM
biTDM[1:30, 60:90]
biTDM[30:60, 60:90]
biTDM[30:60, 30:60]
biTDM[30:60, 1:30]
inspect(biTDM[30:60, 1:30])
sentences[18]
triDTM[1:30, 1:30]
triTDM[1:30, 1:30]
triTDM[1:60, 1:30]
triTDM[30:60, 1:30]
triTDM[30:90, 1:30]
triTDM[30:90, 1:60]
triTDM[30:90, 1:90]
triTDM[30:1200, 1:90]
triTDM[30:120, 1:90]
triTDM[30:150, 1:90]
triTDM[30:180, 1:90]
triTDM[30:210, 1:90]
triTDM[30:240, 1:90]
triTDM[30:270, 1:90]
triTDM[30:270,]
TDM
110921/158129329
biTDM
109623/882787077
triTDM
101248/888668102
biTDM[60:90, 60:90]
biTDM[60:120, 60:90]
biTDM[30:120, 60:90]
biTDM[30:120, 1:30]
biTDM[30:90, 1:30]
biTDM[30:60, 1:30]
biTDM[40:60, 1:30]
biTDM[30:50, 1:30]
inspect(biTDM[30:50, 1:30])
inspect(biTDM[30:50, 10:30])
triTDM[1:100, 1:100]
triTDM[1:50, 1:100]
triTDM[1:25, 1:100]
triTDM[30:50, 1:100]
triTDM[30:50, 1:50]
triTDM[30:50, 1:20]
inspect(triTDM[30:50, 1:20])
test<-c(TDM, biTDM, triTDM)
test
tdm[1:10, 1:100]
TDM[1:10, 1:100]
inspect(TDM[1:10, 1:100])
inspect(TDM[1:10, 8900:9000])
inspect(TDM[1:10, 8900])
inspect(TDM[1:10, 8920:8930])
inspect(TDM[1:10, 8925])
inspect(TDM[1:10, 8926])
inspect(test[1:10, 8926])
inspect(test[1:10, 8925])
1/60
breach <- read.csv('Data/breachmeans.csv')
setwd("/Volumes/triddle/Teaching/Grad stats sem")
breach <- read.csv('Data/breachmeans.csv')
breach <- read.csv('/Data/breachmeans.csv')
ls
ls()
getwd()
breach <- read.csv('Data/breachmeans.csv')
ls
getwd()
setwd("/Volumes/triddle/Teaching/Grad stats sem/StatsWorkshop")
ls
breach <- read.csv('Data/breachmeans.csv')
breach <- read.csv('Data/breachmeans.csv')
dim(breach)
names(breach)
head(breach)
summary(breach)
breach$happy <- rep(c("yes", "no"), each=10)
breach$happy #there's only 20 observations, so we can just print out all of them.
str(breach$happy)
summary(breach$happy)
breach$happy <- as.factor(breach$happy)
breach$happy
str(breach$happy)
summary(breach$happy)
library(reshape2)
breach <- melt(breach,
id.vars = c('subj', 'flourishing', 'swls', 'age', 'exp', 'apride',
'lpride', 'eom', 'rankindiv', 'rank21', 'averank'),
measure.vars = c('locomotion1', 'locomotion2', 'assessment1', 'assessment2',
'promotion1', 'promotion2', 'prevention1', 'prevention2',
'stress1', 'stress2', 'lie2', 'lie1', 'LxA1', 'LxA2'))
str(breach)
summary(breach)
breach[breach$subj == 2,] #read:  from the dataframe breach, give me the rows where subject = 2.  After the comma is where one would select columns.  Since I wanted all the columns, I just left it blank.
breach[1:25, ] #read:  give me the rows 1 through 25, all columns.
breach[breach$variable=='locomotion2', c(1:5, 12)] #read:  give me all the rows where the value for variable is equal to locomotion 2.  only give me columns 1 through 5 and 12 (column 12 is the `variable` column)
breach[41:60, c('subj', 'variable')]
breach$time <- rep(rep(c(1, 2), each = 20), 7)
head(breach)
breach[grep("locomotion", breach$variable)]
breach[grep("locomotion", breach$variable), ]
plot <- ggplot(breach[grep("locomotion", breach$variable), ], aes(x=value, group=time))
plot + geom_histogram()
library(ggplot2)
plot <- ggplot(breach[grep("locomotion", breach$variable), ], aes(x=value, group=time))
plot + geom_histogram()
plot <- ggplot(breach[grep("locomotion", breach$variable), ], aes(x=value, color=time))
plot + geom_histogram()
```
plot + geom_histogram()
```{r}
plot <- ggplot(breach[grep("locomotion", breach$variable), ], aes(x=value, color=time))
plot + ge
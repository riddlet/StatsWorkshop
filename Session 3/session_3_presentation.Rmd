---
title: "Untitled"
output: html_document
---

Maya next computes a series of ANOVAs.  

```{r, eval=FALSE}
## ONE-WAY ANOVAS: main effects
sr.att.mess <- aov(vmessage ~ attitudedum, data=srp)
summary(sr.att.mess) #main effect of attitude

sr.att.rec <- aov(vrecall ~ attitudedum, data=srp)
summary(sr.att.rec) #main effect of attitude

sr.rel.mess <- aov(vmessage ~ relprimedum, data=srp)
summary(sr.rel.mess) #ns

sr.rel.rec <- aov(vrecall ~ relprimedum, data=srp)
summary(sr.rel.rec) #ns

## TWO-WAY ANOVAS: interactions
sr.rel.att.mess <- aov(vmessage ~ attitudedum * relprimedum, data=srp)
summary(sr.rel.att.mess) #main effect of attitude (interaction ns)

sr.rel.att.rec <- aov(vrecall ~ attitudedum * relprimedum, data=srp)
summary(sr.rel.att.rec) #main effect of attitude (interaction ns)
```

This is gonna get ugly.  It's time to consider what we're doing here, and how to specify it.  So, what is an ANOVA?

###ANOVA
It stands for Analysis Of VAriance.  That is, we're taking some observed variability, and figuring out where it comes from.  For simplicity, let's focus on `vmessage`.  I believe this variable reflects the valence of the message that participants produced about someone named *Michael*.  They then sent this message to someone named *Sam*.  For you social cognitive researchers in the room, this is a standard *saying-is-believing* paradigm a-la Tory Higgins.  Regardless, it should be obvious that participants will generate messages which are not exactly equally valenced.  Indeed, we've already seen this in some of the plots we've made.  What we'd like to know is where this variability comes from.  

We've set up an experiment in which we think that two manipulations will induce some variance.  Specifically, we've made it such that some participants think that Sam likes Michael, and others think that Sam dislikes Michael.  We've also primed some people to think of either a recently ended relationship or a relationship that is ongoing.  How much of the variability in `vmessage` is due to the contribution of these two manipulations?

We're thinking of these two variables as completely independent from each other.  That is, if we could compute a correlation coefficient between them, it would be exactly zero.  It turns out, we *can* compute a correlation coefficient!  Look at the data:

```{r}
temp <- data.frame(subj=srp$subj, attitude=as.numeric(srp$attitude), 
                   prime=as.numeric(srp$relprime))
head(temp)
```


We've assigned numeric values to attitude and prime (for the sake of illustration) via `as.numeric()`.  We then put them in a new dataframe called `temp` and looked at the first few rows with `head()`.  What's the correlation between them?  Well, it isn't zero.  

```{r}
cor(as.numeric(srp$attitude), as.numeric(srp$relprime))
```


It's `r round(cor(as.numeric(srp$attitude), as.numeric(srp$relprime)), 5)`.  I know, I know.  *Almost*, right?  Well, almost isn't good enough.  We need zero on the dot.

Why does this matter?  Remember that we're trying to explain the variability in `vmessage`.  We'd like to attribute some of that variability to attitude and some to the prime.  Visually, the variability is usually depicted as a geometric box.

![Variance][img1]

If our two variabiles were totally independent, then we could imagine that they each explain some unique portion of that variance.

![Independent][img2]

However, we know that our two variables are *not* independent.  They are related to each other.  So our situation looks a little more like this:

![Correlated][img3]

What this means for our ANOVA is that we need to figure out how to explain all the variance in our box which is taken up by circles.  In particular, what are we to do with the middle bit?  Is it part of Prime?  Part of attitude?  Part of both?  Neither?  We've inadvertently set up a situation in which our two explanatory variables will be fighting to explain the same piece of variance.

The answer to the question of what to do about the overlapping bit is determined by which Sum of Squares (SS) we select, of which there are three basic types

#### Type I SS
Type I SS breaks up the estimates sequentially:

1.  SS(Prime)
2.  SS(Attitude | Prime)
3.  SS(Prime*Attitude | Prime, Attitude)

In other words, we get all the variance explained by Prime in step 1, then the variance associated with Attitude after we've taken out the .00071 chunk in step 2, and finally, the variance associated with the interaction, after removing Prime and Attitude in step 3.  Below, I've included the interaction term.  I left it out above for pedagogical reasons, but it works in the same way.

To visualize, in step one, we get all the variance associated with Prime:

```{r, message=FALSE}
# Note I'm using a modified version of the colorfulVennPlot package.
# The modified package can be found on my github page.
source('../..//colorfulVennPlot/R/plotVenn3d.R')
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c('#144256', rep('White', 2), rep('#144256', 2),
                             'White', '#144256'), 
                  printvals=F)
```


Then, step two, we take the remaining bit of Attitude:

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c('#144256', rep('#88301B', 2), rep('#144256', 2),
                             'White', '#144256'), printvals=F)
```

In step three, we just take the remaining variance left in the interaction term:

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c('#144256', rep('#88301B', 2), rep('#144256', 2), 
                             '#88691B', '#144256'), printvals=F)
```

When Maya used `aov()`, she was using type I SS.  We can see this by examining what happens if we switch the order in which we enter the terms:

```{r}
sr.rel.att.mess1 <- aov(vmessage ~ attitude * relprime, data=srp)
sr.rel.att.mess2 <- aov(vmessage ~ relprime * attitude, data=srp)
summary(sr.rel.att.mess1) #main effect of attitude (interaction ns)
summary(sr.rel.att.mess2) #main effect of attitude (interaction ns)
```

The differences are not huge, but they are present.  Note, for example, the change in the F-value for our two main effects.  As you can guess, this is not typically what we want when examining an experiment like the one Maya ran.  We usually think of main effects as 'the influence of $IV_1$, holding $IV_2$ constant (sometimes said, 'Influence of $IV_1$, accounting for $IV_2$).

#### Type II SS
Type II resolves this issue, in a fashion.  It gives us the estimates for each main effect, while controlling for the other main effect(s).  That is:  

 - SS(Prime | Attitude)  
 - SS(Attitude | Prime)  
 - SS(Prime * Attitude | Prime, Attitude)  

Let's revisit the those plots of variance accounted for:

```{r, message=FALSE}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c(rep('White', 7)), printvals=F)
```

Now we have a visual representation of our full model.  Now, let's look at what portion of the variance is explained by each of our terms.  First, *SS(Prime | Attitude)*

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c('#144256', rep('White', 2), '#144256', rep('White', 3) ), printvals=F)
```

In plain terms, you can say that we've estimated the amount of total variance accounted for in Prime, removing the variance associated with attitude.  Next *SS(Attitude | Prime)*

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c('#144256', rep('#88301B', 2), '#144256', rep('White', 3) ), printvals=F)
```

Again, in plain terms, we've estimated the amount of total variance accounted for in Attitude, removing the variance associated with Prime.  Finally, *SS(Prime * Attitude | Prime, Attitude)*

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c('#144256', rep('#88301B', 2), '#144256', 'White', 
                             '#88691B', 'White'), printvals=F)
```

Here, we've estimated the amount of variance accounted for by the Prime*Attitude interaction, removing the variance associated with the main effects of Prime and Attitude.  

Why would we want to do this?  Well, as you can see, it gives a lot of power to detect any main effects that might be present, as a large portion of the main effect circles are colored.  However, some of that area is eating into the interaction.  That is, you're saying that you'd rather spend that area detecting a main effect than detecting an interaction.  If this is true, then why are you bothering modeling an interaction in the first place?  If you use type II SS and get a significant interaction term, then that should lead you to question what that means for your main effects, because it isn't clear what to do with the bits that overlap with the interaction term  (should they be part of the interaction?  The main effect?  neither?).

In practice, type II SS are almost never used because of this issue with what's going on in the interaction term.  Some people advocate using this when there's no interaction present, because it gives you better power to detect main effects, but (I think that) this is a relatively small group.

To do a type II in R, we should use the `Anova()` function from the `car` package.  There ~~might be~~ is a way to do it without, but I don't know it, it's probably a bit more complicated:

```{r}
library(car)
sr.rel.att.mess <- aov(vmessage ~ attitude * relprime, data=srp)
a.sr<-Anova(sr.rel.att.mess, type=2)
a.sr
```

It's interesting to note here that the outcome for the two main effects are the same as if we had taken the estimates in step 2 from each of our type I anovas.  One can think about that visually too.  This makes sense, because step 2 in type I anova is the effect of a particular IV, controlling/accounting for the variability in the other one.

#### Type III SS
Type III SS is, historically, the most commonly used in psychology (e.g. the default in SPSS, SAS, & STATA).  This is because we're most typically interested in the effect of every term in our model, controlling for the others.  In our current example:

 - SS(Prime | Attitude, Prime * Attitude)  
 - SS(Attitude | Prime, Prime * Attitude)  
 - SS(Prime * Attitude | Prime, Attitude) 
 
This differs from type II SS because we're now only interested in the bit of variance explained that is *unique* to each particular term.  Visually:

*SS(Prime | Attitude, Prime * Attitude)*

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c(rep('White', 3), '#144256', rep('White', 3) ), 
                  printvals=F)
```

You can see that we're only interested in those pieces of the variance that we are *sure* belong to Prime.  Same for *SS(Prime | Attitude, Prime * Attitude)*:

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c(rep('White', 2), '#88301B', '#144256',
                             rep('White', 3)), printvals=F)
```

Finally, the same holds true for the interaction term, *SS(Prime * Attitude | Attitude, Prime)*:

```{r}
plotVenn3d(c(10, 10, 10, 10, 10, 10, 10), 
                  labels = c('Prime*Attitude', 'Attitude', 'Prime'), 
                  Colors = c(rep('White', 2), '#88301B', '#144256',
                             'White', '#88691B', 'White'), printvals=F)
```

What's the advantage of this approach?  

Well, for one thing, it is relatively conservative.  That is, since we're only looking at the variance that is explained by our model in an unambiguous way, there will never be any confusion over whether some main effect that we're seeing is real, or is actually part of an interaction term (like could potentially happen in type II).  In other words, with type II, we are biased toward detecting main effects, at the expense of detecting any interaction.  This can be especially problematic when the interaction is exactly what is causing the main effect!  A set of results like this, for instance:

```{r}
y<-c(2, 2, 3, 8) #fake means
x<-c('A', 'B', 'A', 'B') # IV 1
z<-c('C', 'C', 'D', 'D') # IV 2
df.plot<-data.frame(x, y, z) # put them in a data frame
plot <- ggplot(df.plot, aes(x=x, y=y, group=z, color=z))
  plot + geom_point(size=7)+
  geom_line()
```

If our data looked like this, and we used a type II sums of squares, we would be more likely to interpret what is clearly an interaction (i.e. x influences Y, but only if you're in category D) as a main effect (i.e. Y is higher for category B than category A).

Note, by the way, that many of these problems just don't exist if we **LOOK AT OUR DATA**, and use hypothesis testing as a way of confirming what we already suspect is happening.  At any rate, how do we go about conducting a type III test in R?  There's one additional step in comparison to the type II.

You see, R is pretty good about automatically handling factors.  This means that we don't typically have to take the extra step of making contrasts (i.e. dummy coded) versions of our variables, as R does it for us.  In order to do this, R relies on a set of internal functions.  As you may or may not remember from some other statistics class, there are any number of ways to create contrasts.  R has some default way of doing it that depends on what kind of factor you're feeding it:

```{r}
getOption('contrasts')
```

R has two types of factors - *unordered*, such as the one's we're dealing with here, and *ordered*, like you might see when administering High, Medium, and Low dosages of a drug.  The above tells us that unordered factors are contrasted using the contr.treatment function, and ordered are contrasted using the contr.poly function.

Let's see how our two variables are coded with the current settings:

```{r}
with(srp, contrasts(attitude))
with(srp, contrasts(relprime))
```

We get a pretty straightforward dummy variable coding.  This works for regression, but when running an ANOVA, we should use effects coding (e.g. making sure the values sum to zero).  The reasoning behind this should probably be saved for a different workshop, but we can easily set it so that this is what we get in R:

```{r}
options(contrasts=c('contr.sum', 'contr.poly'))
with(srp, contrasts(relprime))
```

Note that we're feeding the contrast options two different values - one for contr.sum, and one for contr.poly.  These two values correspond to the two functions used for unordered and ordered factor variable types, respectively.

Once we've done this, we can proceed as we did for a type II, except this time we specify that we are interested in the type III SS:

```{r}
sr.rel.att.mess <- aov(vmessage ~ attitude * relprime, data=srp)
a.sr<-Anova(sr.rel.att.mess, type=3)
a.sr
```

Note that all of this stuff about different sums of squares is really only relevant for unbalanced designs (i.e. differenct observations in different cells).  If you've got a balanced design, then all three methods will give you the same answer, because then your IVs are not correlated, and there's no issue with determinging what variance each variable is explaining.


##### The 3 flavors of sums of squares

*Type I* is sequential.  The estimates you get will depend on the order in which you neter the terms.

*Type II* is biased towards detecting main effects at the expense of interactions, but has more power to do so

*Type III* does not have the bias present in type II, but has the disadvantage of having the least power of any of them (i.e. is the most conservative).  You will almost always be expected to run a type III.

[img1]: ../Images/Var.png
[img2]: ../Images/indep.png
[img3]: ../Images/cor.png

---
title: "Session 2"
output: html_document
---

##Maya's data

Last time, we spent some effort looking at Mark's data.  This time, Maya has been kind enought to donate some of her data for the greater good.  She's already sent me her csv as well as the script she's used for the work.  I'm going to zip through some of her pre-processing first.  I'll stop to highlight a couple of things along the way, though.

```{r, warning=FALSE, message=FALSE}
srp <- read.csv('Data/SR.Past.Rel.12.10.14.csv')
#### EXCLUSIONS ####
#exclude 5 who copied the Michael text verbatim (exclcopied=1)
srp <- srp[ which(is.na(srp$exclcopied) == TRUE ), ]
#exclude 11 who used 'Michael' in their message (didn't follow directions) (exclmich=1)
srp <- srp[ which(is.na(srp$exclmich) == TRUE ), ]

####### COMPUTING VARIABLES ##########
attach(srp)
## rmq
srp$loc <- (rmq_1 + rmq_3 + rmq_4 + rmq_5 + rmq_8 + rmq_16 + rmq_21 + rmq_25 + rmq_28 + rmq_29 + (7-rmq_13) + (7-rmq_24))/12
srp$ass <- ((7-rmq_2) + rmq_6 + rmq_7 + rmq_9 + (7-rmq_10) + rmq_11 + rmq_15 + rmq_19 + rmq_20 + rmq_22 + (7-rmq_27) + rmq_30)/12
srp$locomassess = srp$loc-srp$ass

srp$prom = (6-rfq_1) + rfq_3 + rfq_7 + (6-rfq_9) + rfq_10 + (6-rfq_11)
srp$prev = (6-rfq_2) + (6-rfq_4) + rfq_5 + (6-rfq_6) + (6-rfq_8)
srp$prommprev = srp$prom-srp$prev

# closeness measure
srp$closeness <- (((8-close_1) + (8-close_2) + close_3 + close_4 + (8-close_5) + (8-close_6) + close_7 + close_8)/8)

# ntoBelong
srp$belong = ((6-NTB_1) + (6-NTB_3) + (6-NTB_7) + NTB_2 + NTB_4 + NTB_5 + NTB_6 + NTB_8 + NTB_9 + NTB_10)/10

# Relational Trust
srp$reltrust = (reltrust_1 + reltrust_2 + reltrust_3 + reltrust_4)/4

# Epistemic Trust
srp$epistrust = (epistrust_1 + epistrust_2 + epistrust_3 + epistrust_4)/4

# message trust
srp$messtrust = (messtrust_1 + messtrust_2 + messtrust_3 + messtrust_4)/4

# ECR
srp$anx = (ECR.S_2 + ECR.S_4 + ECR.S_6 + (8-ECR.S_8) + ECR.S_10 + ECR.S_12)
srp$avoid = ((8-ECR.S_1) + ECR.S_3 + (8-ECR.S_5) + ECR.S_7 + ECR.S_9 + ECR.S_11)

#SCS
srp$independence = (((8-SCS_1) + (8-SCS_2) + (8-SCS_3) + (8-SCS_4) + (8-SCS_5) + (8-SCS_6) + (8-SCS_7) + (8-SCS_8) + (8-SCS_9) + (8-SCS_10) + (8-SCS_11)+ (8-SCS_12)+ SCS_13+ SCS_14+ SCS_15+ SCS_16+ SCS_17+ SCS_18+ SCS_19+ SCS_20+ SCS_21+ SCS_22+ SCS_23+ SCS_24)/24)

detach(srp)

#F-Scale
srp$authority = (((srp$F.Scale_1) + (srp$F.Scale_2) + (srp$F.Scale_3) + (srp$F.Scale_4) + (srp$F.Scale_5) + (srp$F.Scale_6) + (srp$F.Scale_7) + 
                    (srp$F.Scale_8) + (srp$F.Scale_9) + (srp$F.Scale_10) + (srp$F.Scale_11)+ (srp$F.Scale_12)+ srp$F.Scale_13+ srp$F.Scale_14 +
                    srp$F.Scale_15+ srp$F.Scale_16+ srp$F.Scale_17+ srp$F.Scale_18+ srp$F.Scale_19+ srp$F.Scale_20+ srp$F.Scale_21+ srp$F.Scale_22)/22)


#### RECODING ####

####renaming manipulations 
#install.packages("reshape")
library(reshape)
library(reshape)
srp <- rename(srp, c(DO.BR.FL_19="relprime")) #dissolved=1, stable=2
srp <- rename(srp, c(DO.BR.FL_20="attitude")) #like=2, dislike=1
```

The first place I'll stop is here.  Maya is creating a couple of new variables, both coded as zero and one.

```{r, echo=FALSE, eval=FALSE}

# dummy coding manipulations
srp$relprimedum[srp$relprime == "Stable Relationship Priming"] <- 0
srp$relprimedum[srp$relprime == "Dissolved Relationship Priming"] <- 1

srp$attitudedum[srp$attitude == "Michael-Dislike"] <- 0
srp$attitudedum[srp$attitude == "Michael-Like"] <- 1
```

However, this might not be necessary.  I haven't looked ahead in her code, but R is pretty good at dealing with factor variables.  It usually knows what to do with them and one usually need not explicitly code them as zero and one.  After doing this, she goes on to create several new subsets of data, as defined by these factor variables, so that she can make some histograms of vmessage and vrecall.

```{r, eval=FALSE}
###### SUBSETTING ####
srlike <- subset(srp, attitudedum==1)
srdislike <- subset(srp, attitudedum==0)
srstable <- subset(srp, relprimedum==0)
srdissolve <- subset(srp, relprimedum==1)

library(ggplot2)
theme_set(theme_bw(base_size = 14)) #for all graphs to have bw background & same font size

ggplot(srlike, aes(x=vmessage)) + 
  geom_density()
ggplot(srdislike, aes(x=vmessage)) + 
  geom_density()
ggplot(srstable, aes(x=vmessage)) + 
  geom_density()
ggplot(srdissolve, aes(x=vmessage)) + 
  geom_density()

ggplot(srlike, aes(x=vrecall)) + 
  geom_density()
ggplot(srdislike, aes(x=vrecall)) + 
  geom_density()
ggplot(srstable, aes(x=vrecall)) + 
  geom_density()
ggplot(srdissolve, aes(x=vrecall)) + 
  geom_density()
```

As far as letting us look at our variables to see if they're normally distributed within these subgroups, this approach works okay.  However, the whole thing could be dramatically simplified, and we could also look to see how these distributions change by group.  This is a similar approach to what we were doing with Mark's data.

```{r}
library(ggplot2)
plot <- ggplot(srp, aes(x=vmessage, color=relprime))
plot + geom_density()
```

I've used the original relprime variable to split the distribution into two density curves.  No need to factor, subset, and create separate plots.  We can do the same for vrecall, and we can also repeat the whole thing for attitude:

```{r}
plot <- ggplot(srp, aes(x=vrecall, color=relprime))
plot + geom_density()
```

```{r}
plot <- ggplot(srp, aes(x=vmessage, color=attitude))
plot + geom_density()
```

```{r}
plot <- ggplot(srp, aes(x=vrecall, color=attitude))
plot + geom_density()
```

Now, since we seem to be splitting the data according to the attitude and relprime variable, I'm guessing these are manipulations or covariates that we're interested in.  Let's simplify this plotting even further by faceting the plot.

```{r}
plot <- ggplot(srp, aes(x=vrecall, color=attitude))
plot + geom_density() + facet_wrap(~relprime)
```

```{r}
plot <- ggplot(srp, aes(x=vmessage, color=attitude))
plot + geom_density() + facet_wrap(~relprime)
```

Oookay, that last one is a bit weird.  Seems like nearly 1/2 of that subset of the sample must have given the same rating?

```{r}
table(srp$vmessage, srp$attitude, srp$relprime)
```

Yeah, this lists 9 people who used .5 in that offending group.  Although unusual, it doesn't seem like anything fishy is going on here.

Next, Maya computes some correlation coefficients.  I think the idea here is right, and you'll want to know these coefficients for writing up the results, but what we're seeking here is *understanding*, right?  I've always felt better about looking at data, rather than figuring out what a one-number summary means.  Time to bust out our good friend, the scatter plot.  First, the overall pattern.  I've added a slight jitter to the points so that we can see if any are plotted right on top of each other (a common problem when dealing with scales which have only a few response options)

```{r}
plot <- ggplot(srp, aes(x=vmessage, y=vrecall))
plot + geom_point(position=position_jitter())
```

A pretty clear, strong, positive relationship.  Now let's look at the same pattern by subsets:

```{r}
plot <- ggplot(srp, aes(x=vmessage, y=vrecall))
plot + geom_point(position=position_jitter()) + facet_grid(attitude~relprime)
```

All of these seem to be similar to the overall pattern.  Maybe Dissolved/Dislike is a slightly weaker relationship than the others?  This also shows that the Like attitude seems to have boosted both variables, don't you think?  See how the points are clustered closer to the upper right for the bottom two plots?  Again, this might not pan out in the proper analyses, but if something is going to show up, that's probably going to be it.

Maya next computes a series of ANOVAs.  

```{r, eval=FALSE}
## ONE-WAY ANOVAS: main effects
sr.att.mess <- aov(vmessage ~ attitudedum, data=srp)
summary(sr.att.mess) #main effect of attitude

sr.att.rec <- aov(vrecall ~ attitudedum, data=srp)
summary(sr.att.rec) #main effect of attitude

sr.rel.mess <- aov(vmessage ~ relprimedum, data=srp)
summary(sr.rel.mess) #ns

sr.rel.rec <- aov(vrecall ~ relprimedum, data=srp)
summary(sr.rel.rec) #ns

## TWO-WAY ANOVAS: interactions
sr.rel.att.mess <- aov(vmessage ~ attitudedum * relprimedum, data=srp)
summary(sr.rel.att.mess) #main effect of attitude (interaction ns)

sr.rel.att.rec <- aov(vrecall ~ attitudedum * relprimedum, data=srp)
summary(sr.rel.att.rec) #main effect of attitude (interaction ns)
```

This is gonna get ugly.  It's time to consider what we're doing here, and how to specify it.  So, what is an ANOVA?

###ANOVA
It stands for Analysis Of VAriance.  That is, we're taking some observed variability, and figuring out where it comes from.  For simplicity, let's focus on `vmessage`.  I believe this variable reflects the valence of the message that participants produced about someone named *Michael*, which they then sent to someone named *Sam*.  We know that the valence will not be the same for everyone.  Indeed, we've already seen this in some of the plots we've made.  What we'd like to know is where this variability comes from.  

We've set up an experiment in which we think that two manipulations will induce some variance.  Specifically, we've made it such that some participants think that Sam likes Michael, and others think that Sam dislikes Michael.  We've also primed some people to think of either a recently ended relationship or a relationship that is ongoing.  How much of the variability in `vmessage` is due to the contribution of these two manipulations?

We're thinking of these two variables as completely independent from each other.  That is, if we could compute a correlation coefficient between them, it would be exactly zero.  It turns out, we *can* compute a correlation coefficient!  Look at the data:

```{r}
temp <- data.frame(subj=srp$subj, attitude=as.numeric(srp$attitude), prime=as.numeric(srp$relprime))
head(temp)
```

We've assigned numeric values to attitude and prime (for the sake of illustration).  What's the correlation between them?  Well, it isn't zero.  It's `r round(cor(as.numeric(srp$attitude), as.numeric(srp$relprime)), 5)`.  I know, I know.  *Almost*, right?  Well, almost isn't good enough.  We need zero on the dot.

Why does this matter?  Remember that we're trying to explain the variability in `vmessage`.  We'd like to attribute some of that variability to attitude and some to the prime.  Visually, the variability is usually depicted as a geometric box.

![Variance][img1]

If our two variabiles were totally independent, then we could imagine that they each explain some unique portion of that variance.

![Independent][img2]

However, we know that our two variables are *not* independent.  They are related to each other.  So our situation looks a little more like this:

![Correlated][img3]

What this means for our ANOVA is that we need to figure out how to explain all the variance in our box which is taken up by circles.  In particular, what are we to do with the middle bit?  Is it part of Prime?  Part of attitude?  Part of both?  Neither?

The answer to these questions is determined by which Sum of Squares (SS) we select, of which there are three basic types

#### Type I SS
Type I SS breaks up the estimates sequentially:

1.  SS(Prime)
2.  SS(Attitude|Prime)
3.  SS(Prime*Attitude|Prime, Attitude)

In other words, we get all the variance explained by Prime in step 1, then the variance associated with Attitude after we've taken out the .00071 chunk in step 2, and finally, the variance associated with the interaction, after removing Prime and Attitude in step 3 (interactions aren't aren't visualized in the venn diagram, but the idea is the same - just add another circle).

When Maya used `aov()`, she was using type I SS.  We can see this by examining what happens if we switch the order in which we enter the terms:

```{r}
sr.rel.att.mess <- aov(vmessage ~ attitudedum * relprimedum, data=srp)
summary(sr.rel.att.mess) #main effect of attitude (interaction ns)

sr.rel.att.mess <- aov(vmessage ~ relprimedum*attitudedum, data=srp)
summary(sr.rel.att.mess) #main effect of attitude (interaction ns)
```

The differences are not huge, but they are present.  Note, for example, the change in the F-value for our two main effects.

#### Type II SS
Type II rectifies some of the weirdness in Type I.  

http://egret.psychol.cam.ac.uk/statistics/R/anova.html
http://stats.stackexchange.com/questions/20452/how-to-interpret-type-i-sequential-anova-and-manova/20455#20455
http://mcfromnz.wordpress.com/2011/03/02/anova-type-iiiiii-ss-explained/
http://faculty.nps.edu/sebuttre/home/R/contrasts.html
http://elderlab.yorku.ca/~elder/teaching/psyc3031/lectures/Lecture%206%20Comparing%20Several%20Means%20-%20ANOVA%20%28GLM%201%29.pdf


[img1]: Images/Var.png
[img2]: Images/indep.png
[img3]: Images/cor.png

